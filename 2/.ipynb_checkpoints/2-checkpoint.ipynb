{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二课 词向量\n",
    "\n",
    "第二课学习目标\n",
    "- 学习词向量的概念\n",
    "- 用Skip-thought模型训练词向量\n",
    "- 学习使用PyTorch dataset和dataloader\n",
    "- 学习定义PyTorch模型\n",
    "- 学习torch.nn中常见的Module\n",
    "    - Embedding\n",
    "- 学习常见的PyTorch operations\n",
    "    - bmm\n",
    "    - logsigmoid\n",
    "- 保存和读取PyTorch模型\n",
    "\n",
    "在这一份notebook中，我们会（尽可能）尝试复现论文[Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)中训练词向量的方法. \n",
    "\n",
    "我们会实现[Skip-gram](https://blog.csdn.net/u010665216/article/details/78721354)模型，并且使用论文中noice contrastive sampling的目标函数。\n",
    "\n",
    "![skip_gram](skip_gram.jpg)\n",
    "\n",
    "这篇论文有很多模型实现的细节，这些细节对于词向量的好坏至关重要。我们虽然无法完全复现论文中的实验结果，主要是由于计算资源等各种细节原因，但是我们还是可以大致展示如何训练词向量。\n",
    "\n",
    "以下是一些我们没有实现的细节\n",
    "- subsampling：参考论文section 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调用PyTorch常用的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基本上所有torch脚本都需要用到\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tud #Pytorch读取训练集需要用到torch.utils.data类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn中大多数layer在torch.nn.funtional中都有一个与之对应的函数。  \n",
    "二者的[区别](https://blog.csdn.net/hawkcici160/article/details/80140059)在于：  \n",
    "- torch. nn.Module中实现layer的都是一个特殊的类 会自动提取可学习的参数  \n",
    "- nn.functional中的函数，更像是纯函数，由def function( )定义，只是进行简单的 数学运算而已。functional中的函数是一个确定的不变的运算公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调用其他需要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他初始设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#调用gpu\n",
    "USE_CUDA=torch.cuda.is_available()\n",
    "\n",
    "#为保证实验结果可以浮现，将各种random seed固定到一个特定的值\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(1)\n",
    "    \n",
    "#设定一些hyper parameters\n",
    "C=3 #nearby words threshold 指定周围3个单词进行预测\n",
    "K=100 #number of negative samples 负样本随机采样数量；每一个正样本对应K个负样本\n",
    "NUM_EPOCHS=2 #The num of epochs of training 迭代轮数\n",
    "MAX_VOCAB_SIZE=30000 #the vocabulary size 词汇表大小\n",
    "BATCH_SIZE=128\n",
    "LEARNING_RATE=0.2 #the initial learning rate\n",
    "EMBEDDING_SIZE=100 #词向量维度\n",
    "\n",
    "#tokenize函数 将文本转化为一个个单词\n",
    "def word_tokenize(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理及相关操作\n",
    "- 从文本文件中读取所有的文字，通过这些文本创建一个vocabulary\n",
    "- 由于单词数量可能太大，我们只选取最常见的MAX_VOCAB_SIZE个单词\n",
    "- 我们添加一个UNK单词表示除MAX_VOCAB_SIZE个单词外其他所有不常见的单词\n",
    "- 我们需要记录单词到index的mapping，以及index到单词的mapping，单词的count，单词的(normalized) frequency，以及单词总数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0),\n",
       " ('of', 1),\n",
       " ('and', 2),\n",
       " ('one', 3),\n",
       " ('in', 4),\n",
       " ('a', 5),\n",
       " ('to', 6),\n",
       " ('zero', 7),\n",
       " ('nine', 8),\n",
       " ('two', 9),\n",
       " ('is', 10),\n",
       " ('as', 11),\n",
       " ('eight', 12),\n",
       " ('for', 13),\n",
       " ('s', 14),\n",
       " ('five', 15),\n",
       " ('three', 16),\n",
       " ('was', 17),\n",
       " ('by', 18),\n",
       " ('that', 19),\n",
       " ('four', 20),\n",
       " ('six', 21),\n",
       " ('seven', 22),\n",
       " ('with', 23),\n",
       " ('on', 24),\n",
       " ('are', 25),\n",
       " ('it', 26),\n",
       " ('from', 27),\n",
       " ('or', 28),\n",
       " ('his', 29),\n",
       " ('an', 30),\n",
       " ('be', 31),\n",
       " ('this', 32),\n",
       " ('which', 33),\n",
       " ('at', 34),\n",
       " ('he', 35),\n",
       " ('not', 36),\n",
       " ('also', 37),\n",
       " ('have', 38),\n",
       " ('were', 39),\n",
       " ('has', 40),\n",
       " ('but', 41),\n",
       " ('other', 42),\n",
       " ('their', 43),\n",
       " ('its', 44),\n",
       " ('they', 45),\n",
       " ('first', 46),\n",
       " ('some', 47),\n",
       " ('had', 48),\n",
       " ('more', 49),\n",
       " ('all', 50),\n",
       " ('can', 51),\n",
       " ('most', 52),\n",
       " ('been', 53),\n",
       " ('such', 54),\n",
       " ('many', 55),\n",
       " ('who', 56),\n",
       " ('new', 57),\n",
       " ('there', 58),\n",
       " ('used', 59),\n",
       " ('after', 60),\n",
       " ('when', 61),\n",
       " ('time', 62),\n",
       " ('into', 63),\n",
       " ('these', 64),\n",
       " ('only', 65),\n",
       " ('american', 66),\n",
       " ('see', 67),\n",
       " ('may', 68),\n",
       " ('than', 69),\n",
       " ('i', 70),\n",
       " ('world', 71),\n",
       " ('would', 72),\n",
       " ('b', 73),\n",
       " ('no', 74),\n",
       " ('d', 75),\n",
       " ('however', 76),\n",
       " ('between', 77),\n",
       " ('about', 78),\n",
       " ('over', 79),\n",
       " ('states', 80),\n",
       " ('years', 81),\n",
       " ('people', 82),\n",
       " ('if', 83),\n",
       " ('war', 84),\n",
       " ('during', 85),\n",
       " ('known', 86),\n",
       " ('united', 87),\n",
       " ('called', 88),\n",
       " ('use', 89),\n",
       " ('th', 90),\n",
       " ('system', 91),\n",
       " ('often', 92),\n",
       " ('so', 93),\n",
       " ('state', 94),\n",
       " ('history', 95),\n",
       " ('will', 96),\n",
       " ('up', 97),\n",
       " ('while', 98),\n",
       " ('where', 99)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#读取文件\n",
    "with open('./text8/text8.train.txt','r') as fi:\n",
    "    text=fi.read()\n",
    "    \n",
    "# len(text)\n",
    "\n",
    "#分词\n",
    "#str.lower()将str中大写转化为小写\n",
    "text=[w for w in word_tokenize(text.lower())]\n",
    "\n",
    "#将出现频率最高的MAX_VOCAB_SIZE-1个单词取出来，以字典的形式存储(包含每个单词出现次数)\n",
    "#-1留给UNK单词\n",
    "#collection.Counter(text): 计算每个元素出现个数 返回counter对象\n",
    "#Counter(text).most_common(N): 找到text中出现最多的前N个元素\n",
    "#https://zhuanlan.zhihu.com/p/350899229\n",
    "vocab=dict(Counter(text).most_common(MAX_VOCAB_SIZE-1))\n",
    "#将UNK单词添加进vocab\n",
    "#UNK出现次数=总单词出现次数-常见单词出现次数\n",
    "#dic.values() 返回字典中所有值所构成的对象\n",
    "vocab['<unk>']=len(text)-np.sum(list(vocab.values()))\n",
    "\n",
    "#从vocab中取出所有单词\n",
    "idx_to_word=[word for word in vocab.keys()]\n",
    "\n",
    "#以字典的形式取得单词及其对应的索引\n",
    "#enumerate: 接收一个可遍历的数据对象['a','b','c'] 返回索引与对象的组合[(0,'a'),(1,'b'),(2,'c')]\n",
    "#索引值与单词出现次数相反，最常见单词索引为0。\n",
    "word_to_idx={word:i for i,word in enumerate(idx_to_word)}\n",
    "\n",
    "list(word_to_idx.items())[:100]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pytorch",
   "language": "python",
   "name": "venv_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
