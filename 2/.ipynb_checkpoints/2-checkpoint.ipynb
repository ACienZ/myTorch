{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二课 词向量\n",
    "\n",
    "第二课学习目标\n",
    "- 学习词向量的概念\n",
    "- 用Skip-thought模型训练词向量\n",
    "- 学习使用PyTorch dataset和dataloader\n",
    "- 学习定义PyTorch模型\n",
    "- 学习torch.nn中常见的Module\n",
    "    - Embedding\n",
    "- 学习常见的PyTorch operations\n",
    "    - bmm\n",
    "    - logsigmoid\n",
    "- 保存和读取PyTorch模型\n",
    "\n",
    "在这一份notebook中，我们会（尽可能）尝试复现论文[Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)中训练词向量的方法. \n",
    "\n",
    "我们会实现[Skip-gram](https://blog.csdn.net/u010665216/article/details/78721354)模型，并且使用论文中noice contrastive sampling的目标函数。\n",
    "\n",
    "![skip_gram](skip_gram.jpg)\n",
    "\n",
    "这篇论文有很多模型实现的细节，这些细节对于词向量的好坏至关重要。我们虽然无法完全复现论文中的实验结果，主要是由于计算资源等各种细节原因，但是我们还是可以大致展示如何训练词向量。\n",
    "\n",
    "以下是一些我们没有实现的细节\n",
    "- subsampling：参考论文section 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调用PyTorch常用的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基本上所有torch脚本都需要用到\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tud #Pytorch读取训练集需要用到torch.utils.data类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn中大多数layer在torch.nn.funtional中都有一个与之对应的函数。  \n",
    "二者的[区别](https://blog.csdn.net/hawkcici160/article/details/80140059)在于：  \n",
    "- torch. nn.Module中实现layer的都是一个特殊的类 会自动提取可学习的参数  \n",
    "- nn.functional中的函数，更像是纯函数，由def function( )定义，只是进行简单的 数学运算而已。functional中的函数是一个确定的不变的运算公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调用其他需要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他初始设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#调用gpu\n",
    "USE_CUDA=torch.cuda.is_available()\n",
    "\n",
    "#为保证实验结果可以浮现，将各种random seed固定到一个特定的值\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(1)\n",
    "    \n",
    "#设定一些hyper parameters\n",
    "C=3 #nearby words threshold 指定前后3个单词进行预测\n",
    "K=100 #number of negative samples 负样本随机采样数量；每一个正样本对应K个负样本\n",
    "NUM_EPOCHS=2 #The num of epochs of training 迭代轮数\n",
    "MAX_VOCAB_SIZE=30000 #the vocabulary size 词汇表大小\n",
    "BATCH_SIZE=128\n",
    "LEARNING_RATE=0.2 #the initial learning rate\n",
    "EMBEDDING_SIZE=100 #词向量维度\n",
    "\n",
    "#tokenize函数 将文本转化为一个个单词\n",
    "def word_tokenize(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理及相关操作\n",
    "- 从文本文件中读取所有的文字，通过这些文本创建一个vocabulary\n",
    "- 由于单词数量可能太大，我们只选取最常见的MAX_VOCAB_SIZE个单词\n",
    "- 我们添加一个UNK单词表示除MAX_VOCAB_SIZE个单词外其他所有不常见的单词\n",
    "- 我们需要记录单词到index的mapping，以及index到单词的mapping，单词的count，单词的(normalized) frequency，以及单词总数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#读取文件\n",
    "with open('./text8/text8.train.txt','r') as fi:\n",
    "    text=fi.read()\n",
    "    \n",
    "# len(text)\n",
    "\n",
    "#分词\n",
    "#str.lower()将str中大写转化为小写\n",
    "text=[w for w in word_tokenize(text.lower())]\n",
    "\n",
    "#将出现频率最高的MAX_VOCAB_SIZE-1个单词取出来，以字典的形式存储(包含每个单词出现次数)\n",
    "#-1留给UNK单词\n",
    "#collection.Counter(text): 计算每个元素出现个数 返回counter对象\n",
    "#Counter(text).most_common(N): 找到text中出现最多的前N个元素\n",
    "#https://zhuanlan.zhihu.com/p/350899229\n",
    "vocab=dict(Counter(text).most_common(MAX_VOCAB_SIZE-1))\n",
    "#将UNK单词添加进vocab\n",
    "#UNK出现次数=总单词出现次数-常见单词出现次数\n",
    "#dic.values() 返回字典中所有值所构成的对象\n",
    "vocab['<unk>']=len(text)-np.sum(list(vocab.values()))\n",
    "\n",
    "#从vocab中取出所有单词\n",
    "idx_to_word=[word for word in vocab.keys()]\n",
    "\n",
    "#以字典的形式取得单词及其对应的索引\n",
    "#enumerate: 接收一个可遍历的数据对象['a','b','c'] 返回索引与对象的组合[(0,'a'),(1,'b'),(2,'c')]\n",
    "#索引值与单词出现次数相反，最常见单词索引为0。\n",
    "word_to_idx={word:i for i,word in enumerate(idx_to_word)}\n",
    "\n",
    "# list(word_to_idx.items())[:100]\n",
    "\n",
    "#计算每个单词频率 负采样时需要使用\n",
    "#获得所有单词出现的次数\n",
    "word_counts=np.array([count for count in vocab.values()], dtype=np.float32)\n",
    "#计算所有单词的频率\n",
    "word_freqs=word_counts/np.sum(word_counts)\n",
    "#论文Distributed Representations of Word...中频率取了3/4次方\n",
    "word_freqs=word_freqs**(3./4.)\n",
    "#重新normalize 重新计算所有单词频率 类似softmax\n",
    "word_freqs=word_freqs/np.sum(word_freqs)\n",
    "\n",
    "#检查单词数为MAX_VOCAB_SIZE\n",
    "VOCAB_SIZE=len(idx_to_word)\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现Dataloader\n",
    "\n",
    "一个dataloader需要以下内容：\n",
    "\n",
    "- 把所有text编码成数字，然后用subsampling预处理这些文字。\n",
    "- 保存vocabulary，单词count，normalized word frequency\n",
    "- 每个iteration sample一个中心词\n",
    "- 根据当前的中心词返回context单词\n",
    "- 根据中心词sample一些negative单词\n",
    "- 返回单词的counts\n",
    "\n",
    "这里有一个好的tutorial介绍如何使用[PyTorch dataloader](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
    "为了使用dataloader，我们需要定义以下两个function:\n",
    "\n",
    "- ```__len__``` function需要返回整个数据集中有多少个item\n",
    "- ```__get__``` 根据给定的index返回一个item\n",
    "\n",
    "有了dataloader之后，我们可以轻松随机打乱整个数据集，拿到一个batch的数据等等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbeddingDataset(tud.Dataset):\n",
    "    def __init__(self, text, word_to_idx, idx_to_word, word_freqs, word_counts):\n",
    "        #初始化模型\n",
    "        #super(WordEmbeddingDataset, self).__init__()\n",
    "        super().__init__()\n",
    "        \n",
    "        #顺序存储每个在text中word的word_to_idx中的索引(序号)，\n",
    "        #如果word不在word_to_idx中（属于unk）则存储unk在word_to_idx中对应的序号\n",
    "        self.text_encoded=[word_to_idx.get(word,word_to_idx['<unk>']) for word in text]\n",
    "        #转化为int型LongTensor\n",
    "        self.text_encoded=torch.LongTensor(self.text_encoded)\n",
    "        \n",
    "        #将输入的参数初始化为torch tensor\n",
    "        self.word_to_idx=word_to_idx\n",
    "        self.idx_to_word=idx_to_word #类中没有使用\n",
    "        self.word_freqs=torch.Tensor(word_freqs)\n",
    "        self.word_counts=torch.Tensor(word_counts) #类中没有时使用\n",
    "        \n",
    "    #数据集一共有多少个item\n",
    "    def __len__(self):\n",
    "        return len(self.text_encoded)\n",
    "    \n",
    "    #提供一个index 返回一串训练数据\n",
    "    #index为训练数据集中每个单词对应的序号,即text_encoded中每个元素下标\n",
    "    def __getitem__(self, index):\n",
    "        #中心词 根据index可获得text中index位置的词(以数字表示)\n",
    "        center_word=self.text_encoded[index]\n",
    "        \n",
    "        #周围词 为中心词前C个词与后C个词\n",
    "        #pos_indices_list存储了中心词的周围词对应的序号\n",
    "        #注意当index=0,1,2, len(self.text_encoded)-3,len(self.text_encoded)-2,len(self.text_encoded)-1时,\n",
    "        #pos_indices_serialNumber的范围会超出text_encoded的范围\n",
    "        pos_indices_serialNumber=list(range(index-C,index))+list(range(index+1,index+1+C))\n",
    "        #print(pos_indices_serialNumber)\n",
    "        \n",
    "        #所以需要对pos_indices_serialNumber中的元素逐个同text_encoded的长度取余,\n",
    "        #个人认为这一步的合理性存在疑问\n",
    "        #将训练集最后的几个词作为最开始几个中心词的周围词/将训练集最初的几个词作为最后几个中心词的周围词\n",
    "        #都没有合理性\n",
    "        pos_indices_new_serialNumber=[i % len(self.text_encoded) for i in pos_indices_serialNumber]\n",
    "        #print(pos_indices_new_serialNumber)\n",
    "        #print(type(pos_indices_new_serialNumber))\n",
    "        \n",
    "        #由pos_indices_new_serialNumber获得text中对应位置的词(以数字表示)\n",
    "        #text_encoded为Tensor,可以接收一组数组作为序号,返回序号对应的元素\n",
    "        pos_words=self.text_encoded[pos_indices_new_serialNumber]\n",
    "        #print(type(self.text_encoded))\n",
    "        #print(pos_words)\n",
    "        \n",
    "        #用于negative sampling\n",
    "        #参考https://towardsdatascience.com/nlp-101-negative-sampling-and-glove-936c88f3bc68\n",
    "        \n",
    "        #torch.multinomial\n",
    "        #https://pytorch.org/docs/stable/generated/torch.multinomial.html\n",
    "        #作用是对self.word_freqs做K * pos_words.shape[0]次取值，输出的是self.word_freqs对应的下标。\n",
    "        #取样方式采用有放回的采样，并且self.word_freqs数值越大，取样概率越大。\n",
    "        #每个正确的单词采样K个，pos_words.shape[0]是正确单词数量,pos_words.shape[0]的值为6\n",
    "        neg_words=torch.multinomial(self.word_freqs, K*pos_words.shape[0], True)\n",
    "        #print(neg_words)\n",
    "        \n",
    "        return center_word, pos_words, neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义Dataset\n",
    "dataset=WordEmbeddingDataset(text, word_to_idx, idx_to_word, word_freqs, word_counts)\n",
    "\n",
    "#定义dataloader\n",
    "#num_workers: 线程数量\n",
    "#当num_workers=4时,调用next(iter(dataloader))时,会报错:[Errno 32] Broken pipe \n",
    "#原因可能为内存溢出 参考:https://blog.csdn.net/qq_33666011/article/details/81873217\n",
    "#解决方案为将workers设为0\n",
    "# dataloader=tud.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "dataloader=tud.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-b9ea22244e16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# print([(k,v) for i, (k, v) in enumerate(dict(Counter(dtet)).items()) if i <100])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\venv_pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    277\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_SingleProcessDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venv_pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    717\u001b[0m             \u001b[1;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 719\u001b[1;33m             \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    720\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\python37\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\python37\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\python37\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\python37\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\python37\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "#测试class定义是否存在bug\n",
    "#这一系列测试最好将class WordEmbeddingDataset的return注释掉再测试\n",
    "# print(dataset.__getitem__(0))\n",
    "# print(dataset.__getitem__(1))\n",
    "# print(dataset.__getitem__(2))\n",
    "# print(dataset.__getitem__(3))\n",
    "# print(dataset.__getitem__(len(dte)-3))\n",
    "# print(dataset.__getitem__(len(dte)-2))\n",
    "# print(dataset.__getitem__(len(dte)-1))\n",
    "\n",
    "# dte=dataset.text_encoded\n",
    "# print(dte)\n",
    "# print(len(dte))\n",
    "# dtet=dte.tolist()\n",
    "# print(dtet[:100])\n",
    "# print(type(dtet[0]))\n",
    "# print(dict(Counter(dtet)))\n",
    "# print([(k,v) for i, (k, v) in enumerate(dict(Counter(dtet)).items()) if i <100])\n",
    "\n",
    "next(iter(dataloader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pytorch",
   "language": "python",
   "name": "venv_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
