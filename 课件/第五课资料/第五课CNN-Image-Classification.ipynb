{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "print(\"PyTorch Version: \",torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们定义一个基于ConvNet的简单神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLL loss的定义\n",
    "\n",
    "$\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
    "        l_n = - w_{y_n} x_{n,y_n}, \\quad\n",
    "        w_{c} = \\text{weight}[c] \\cdot \\mathbb{1}\\{c \\not= \\text{ignore\\_index}\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, log_interval=100):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset), \n",
    "                100. * batch_idx / len(train_loader), loss.item()\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0.000000%)]\tLoss: 2.297938\n",
      "Train Epoch: 1 [3200/60000 (5.333333%)]\tLoss: 0.567845\n",
      "Train Epoch: 1 [6400/60000 (10.666667%)]\tLoss: 0.206370\n",
      "Train Epoch: 1 [9600/60000 (16.000000%)]\tLoss: 0.094653\n",
      "Train Epoch: 1 [12800/60000 (21.333333%)]\tLoss: 0.180530\n",
      "Train Epoch: 1 [16000/60000 (26.666667%)]\tLoss: 0.041645\n",
      "Train Epoch: 1 [19200/60000 (32.000000%)]\tLoss: 0.135092\n",
      "Train Epoch: 1 [22400/60000 (37.333333%)]\tLoss: 0.054001\n",
      "Train Epoch: 1 [25600/60000 (42.666667%)]\tLoss: 0.111863\n",
      "Train Epoch: 1 [28800/60000 (48.000000%)]\tLoss: 0.059039\n",
      "Train Epoch: 1 [32000/60000 (53.333333%)]\tLoss: 0.089227\n",
      "Train Epoch: 1 [35200/60000 (58.666667%)]\tLoss: 0.186015\n",
      "Train Epoch: 1 [38400/60000 (64.000000%)]\tLoss: 0.093208\n",
      "Train Epoch: 1 [41600/60000 (69.333333%)]\tLoss: 0.077090\n",
      "Train Epoch: 1 [44800/60000 (74.666667%)]\tLoss: 0.038075\n",
      "Train Epoch: 1 [48000/60000 (80.000000%)]\tLoss: 0.036247\n",
      "Train Epoch: 1 [51200/60000 (85.333333%)]\tLoss: 0.052358\n",
      "Train Epoch: 1 [54400/60000 (90.666667%)]\tLoss: 0.013201\n",
      "Train Epoch: 1 [57600/60000 (96.000000%)]\tLoss: 0.036660\n",
      "\n",
      "Test set: Average loss: 0.0644, Accuracy: 9802/10000 (98%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0.000000%)]\tLoss: 0.054402\n",
      "Train Epoch: 2 [3200/60000 (5.333333%)]\tLoss: 0.032239\n",
      "Train Epoch: 2 [6400/60000 (10.666667%)]\tLoss: 0.092350\n",
      "Train Epoch: 2 [9600/60000 (16.000000%)]\tLoss: 0.058544\n",
      "Train Epoch: 2 [12800/60000 (21.333333%)]\tLoss: 0.029762\n",
      "Train Epoch: 2 [16000/60000 (26.666667%)]\tLoss: 0.012521\n",
      "Train Epoch: 2 [19200/60000 (32.000000%)]\tLoss: 0.101891\n",
      "Train Epoch: 2 [22400/60000 (37.333333%)]\tLoss: 0.127773\n",
      "Train Epoch: 2 [25600/60000 (42.666667%)]\tLoss: 0.009259\n",
      "Train Epoch: 2 [28800/60000 (48.000000%)]\tLoss: 0.013482\n",
      "Train Epoch: 2 [32000/60000 (53.333333%)]\tLoss: 0.039676\n",
      "Train Epoch: 2 [35200/60000 (58.666667%)]\tLoss: 0.016707\n",
      "Train Epoch: 2 [38400/60000 (64.000000%)]\tLoss: 0.168691\n",
      "Train Epoch: 2 [41600/60000 (69.333333%)]\tLoss: 0.056318\n",
      "Train Epoch: 2 [44800/60000 (74.666667%)]\tLoss: 0.008174\n",
      "Train Epoch: 2 [48000/60000 (80.000000%)]\tLoss: 0.075149\n",
      "Train Epoch: 2 [51200/60000 (85.333333%)]\tLoss: 0.205798\n",
      "Train Epoch: 2 [54400/60000 (90.666667%)]\tLoss: 0.019762\n",
      "Train Epoch: 2 [57600/60000 (96.000000%)]\tLoss: 0.012056\n",
      "\n",
      "Test set: Average loss: 0.0464, Accuracy: 9850/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(53113)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "batch_size = test_batch_size = 32\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./mnist_data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./mnist_data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "lr = 0.01\n",
    "momentum = 0.5\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "epochs = 2\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n",
    "\n",
    "save_model = True\n",
    "if (save_model):\n",
    "    torch.save(model.state_dict(),\"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n",
      "Train Epoch: 1 [0/60000 (0.000000%)]\tLoss: 2.279603\n",
      "Train Epoch: 1 [3200/60000 (5.333333%)]\tLoss: 0.962251\n",
      "Train Epoch: 1 [6400/60000 (10.666667%)]\tLoss: 1.019635\n",
      "Train Epoch: 1 [9600/60000 (16.000000%)]\tLoss: 0.544330\n",
      "Train Epoch: 1 [12800/60000 (21.333333%)]\tLoss: 0.629807\n",
      "Train Epoch: 1 [16000/60000 (26.666667%)]\tLoss: 0.514437\n",
      "Train Epoch: 1 [19200/60000 (32.000000%)]\tLoss: 0.555741\n",
      "Train Epoch: 1 [22400/60000 (37.333333%)]\tLoss: 0.528186\n",
      "Train Epoch: 1 [25600/60000 (42.666667%)]\tLoss: 0.656440\n",
      "Train Epoch: 1 [28800/60000 (48.000000%)]\tLoss: 0.294654\n",
      "Train Epoch: 1 [32000/60000 (53.333333%)]\tLoss: 0.293626\n",
      "Train Epoch: 1 [35200/60000 (58.666667%)]\tLoss: 0.227645\n",
      "Train Epoch: 1 [38400/60000 (64.000000%)]\tLoss: 0.473842\n",
      "Train Epoch: 1 [41600/60000 (69.333333%)]\tLoss: 0.724678\n",
      "Train Epoch: 1 [44800/60000 (74.666667%)]\tLoss: 0.519580\n",
      "Train Epoch: 1 [48000/60000 (80.000000%)]\tLoss: 0.465854\n",
      "Train Epoch: 1 [51200/60000 (85.333333%)]\tLoss: 0.378200\n",
      "Train Epoch: 1 [54400/60000 (90.666667%)]\tLoss: 0.503832\n",
      "Train Epoch: 1 [57600/60000 (96.000000%)]\tLoss: 0.616502\n",
      "\n",
      "Test set: Average loss: 0.4365, Accuracy: 8425/10000 (84%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0.000000%)]\tLoss: 0.385171\n",
      "Train Epoch: 2 [3200/60000 (5.333333%)]\tLoss: 0.329045\n",
      "Train Epoch: 2 [6400/60000 (10.666667%)]\tLoss: 0.308792\n",
      "Train Epoch: 2 [9600/60000 (16.000000%)]\tLoss: 0.360471\n",
      "Train Epoch: 2 [12800/60000 (21.333333%)]\tLoss: 0.445865\n",
      "Train Epoch: 2 [16000/60000 (26.666667%)]\tLoss: 0.357145\n",
      "Train Epoch: 2 [19200/60000 (32.000000%)]\tLoss: 0.376523\n",
      "Train Epoch: 2 [22400/60000 (37.333333%)]\tLoss: 0.389735\n",
      "Train Epoch: 2 [25600/60000 (42.666667%)]\tLoss: 0.308655\n",
      "Train Epoch: 2 [28800/60000 (48.000000%)]\tLoss: 0.352300\n",
      "Train Epoch: 2 [32000/60000 (53.333333%)]\tLoss: 0.499613\n",
      "Train Epoch: 2 [35200/60000 (58.666667%)]\tLoss: 0.282398\n",
      "Train Epoch: 2 [38400/60000 (64.000000%)]\tLoss: 0.330232\n",
      "Train Epoch: 2 [41600/60000 (69.333333%)]\tLoss: 0.430427\n",
      "Train Epoch: 2 [44800/60000 (74.666667%)]\tLoss: 0.406084\n",
      "Train Epoch: 2 [48000/60000 (80.000000%)]\tLoss: 0.443538\n",
      "Train Epoch: 2 [51200/60000 (85.333333%)]\tLoss: 0.348947\n",
      "Train Epoch: 2 [54400/60000 (90.666667%)]\tLoss: 0.424920\n",
      "Train Epoch: 2 [57600/60000 (96.000000%)]\tLoss: 0.231494\n",
      "\n",
      "Test set: Average loss: 0.3742, Accuracy: 8652/10000 (87%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(53113)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "batch_size = test_batch_size = 32\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST('./fashion_mnist_data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST('./fashion_mnist_data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "lr = 0.01\n",
    "momentum = 0.5\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "epochs = 2\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n",
    "\n",
    "save_model = True\n",
    "if (save_model):\n",
    "    torch.save(model.state_dict(),\"fashion_mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN模型的迁移学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 很多时候当我们需要训练一个新的图像分类任务，我们不会完全从一个随机的模型开始训练，而是利用_预训练_的模型来加速训练的过程。我们经常使用在`ImageNet`上的预训练模型。\n",
    "- 这是一种transfer learning的方法。我们常用以下两种方法做迁移学习。\n",
    "    - fine tuning: 从一个预训练模型开始，我们改变一些模型的架构，然后继续训练整个模型的参数。\n",
    "    - feature extraction: 我们不再改变与训练模型的参数，而是只更新我们改变过的部分模型参数。我们之所以叫它feature extraction是因为我们把预训练的CNN模型当做一个特征提取模型，利用提取出来的特征做来完成我们的训练任务。\n",
    "    \n",
    "以下是构建和训练迁移学习模型的基本步骤：\n",
    "- 初始化预训练模型\n",
    "- 把最后一层的输出层改变成我们想要分的类别总数\n",
    "- 定义一个optimizer来更新参数\n",
    "- 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torchvision Version:  0.2.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据\n",
    "------\n",
    "\n",
    "我们会使用*hymenoptera_data*数据集，[下载](https://download.pytorch.org/tutorial/hymenoptera_data.zip).\n",
    "\n",
    "这个数据集包括两类图片, **bees** 和 **ants**, 这些数据都被处理成了可以使用`ImageFolder <https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.ImageFolder>`来读取的格式。我们只需要把``data_dir``设置成数据的根目录，然后把``model_name``设置成我们想要使用的与训练模型：\n",
    "::\n",
    "   [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "\n",
    "其他的参数有：\n",
    "- ``num_classes``表示数据集分类的类别数\n",
    "- ``batch_size``\n",
    "- ``num_epochs``\n",
    "- ``feature_extract``表示我们训练的时候使用fine tuning还是feature extraction方法。如果``feature_extract = False``，整个模型都会被同时更新。如果``feature_extract = True``，只有模型的最后一层被更新。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms \n",
    "#   to the ImageFolder structure\n",
    "data_dir = \"./hymenoptera_data\"\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"resnet\"\n",
    "# Number of classes in the dataset\n",
    "num_classes = 2\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 32\n",
    "# Number of epochs to train for \n",
    "num_epochs = 15\n",
    "# Flag for feature extracting. When False, we finetune the whole model, \n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=5):\n",
    "    since = time.time()\n",
    "    val_acc_history = []\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch, num_epochs-1))\n",
    "        print(\"-\"*10)\n",
    "        \n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            running_loss = 0.\n",
    "            running_corrects = 0.\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else: \n",
    "                model.eval()\n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                with torch.autograd.set_grad_enabled(phase==\"train\"):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                if phase == \"train\":\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds.view(-1) == labels.view(-1)).item()\n",
    "            \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects / len(dataloaders[phase].dataset)\n",
    "       \n",
    "            print(\"{} Loss: {} Acc: {}\".format(phase, epoch_loss, epoch_acc))\n",
    "            if phase == \"val\" and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == \"val\":\n",
    "                val_acc_history.append(epoch_acc)\n",
    "            \n",
    "        print()\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print(\"Training compete in {}m {}s\".format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print(\"Best val Acc: {}\".format(best_acc))\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# it = iter(dataloaders_dict[\"train\"])\n",
    "# inputs, labels = next(it)\n",
    "# for inputs, labels in dataloaders_dict[\"train\"]:\n",
    "#     print(labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    if model_name == \"resnet\":\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "        \n",
    "    return model_ft, input_size\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读入数据\n",
    "---------\n",
    "\n",
    "现在我们知道了模型输入的size，我们就可以把数据预处理成相应的格式。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_imgs = datasets.ImageFolder(os.path.join(data_dir, \"train\"), transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ]))\n",
    "loader = torch.utils.data.DataLoader(all_imgs, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = next(iter(loader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unloader = transforms.ToPILImage()  # reconvert into PIL image\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "def imshow(tensor, title=None):\n",
    "    image = tensor.cpu().clone()  # we clone the tensor to not do changes on it\n",
    "    image = image.squeeze(0)      # remove the fake batch dimension\n",
    "    image = unloader(image)\n",
    "    plt.imshow(image)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001) # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "imshow(img[31], title='Image')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "data_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are \n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n",
      "train Loss: 0.2623850886450439 Acc: 0.8975409836065574\n",
      "val Loss: 0.22199168762350394 Acc: 0.9215686274509803\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n",
      "train Loss: 0.20775875546893136 Acc: 0.9262295081967213\n",
      "val Loss: 0.21329789413930544 Acc: 0.9215686274509803\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n",
      "train Loss: 0.24463887243974405 Acc: 0.9098360655737705\n",
      "val Loss: 0.2308054333613589 Acc: 0.9215686274509803\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n",
      "train Loss: 0.2108444703406975 Acc: 0.930327868852459\n",
      "val Loss: 0.20637644174831365 Acc: 0.954248366013072\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n",
      "train Loss: 0.22102872954040279 Acc: 0.9221311475409836\n",
      "val Loss: 0.19902625017695957 Acc: 0.9281045751633987\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n",
      "train Loss: 0.22044393127081824 Acc: 0.9221311475409836\n",
      "val Loss: 0.2212505256818011 Acc: 0.9281045751633987\n",
      "\n",
      "Epoch 6/14\n",
      "----------\n",
      "train Loss: 0.1636357441788814 Acc: 0.9467213114754098\n",
      "val Loss: 0.1969745449380937 Acc: 0.934640522875817\n",
      "\n",
      "Epoch 7/14\n",
      "----------\n",
      "train Loss: 0.1707800094221459 Acc: 0.9385245901639344\n",
      "val Loss: 0.20569930824578977 Acc: 0.934640522875817\n",
      "\n",
      "Epoch 8/14\n",
      "----------\n",
      "train Loss: 0.18224841185280535 Acc: 0.9344262295081968\n",
      "val Loss: 0.192565394480244 Acc: 0.9411764705882353\n",
      "\n",
      "Epoch 9/14\n",
      "----------\n",
      "train Loss: 0.17762072372143387 Acc: 0.9385245901639344\n",
      "val Loss: 0.19549715163466197 Acc: 0.9411764705882353\n",
      "\n",
      "Epoch 10/14\n",
      "----------\n",
      "train Loss: 0.19314993575948183 Acc: 0.9180327868852459\n",
      "val Loss: 0.2000840900380627 Acc: 0.934640522875817\n",
      "\n",
      "Epoch 11/14\n",
      "----------\n",
      "train Loss: 0.21551114418467537 Acc: 0.9057377049180327\n",
      "val Loss: 0.18960770005299374 Acc: 0.934640522875817\n",
      "\n",
      "Epoch 12/14\n",
      "----------\n",
      "train Loss: 0.1847396502729322 Acc: 0.9426229508196722\n",
      "val Loss: 0.1871058808432685 Acc: 0.9411764705882353\n",
      "\n",
      "Epoch 13/14\n",
      "----------\n",
      "train Loss: 0.17342406132670699 Acc: 0.9508196721311475\n",
      "val Loss: 0.20636656588199093 Acc: 0.9215686274509803\n",
      "\n",
      "Epoch 14/14\n",
      "----------\n",
      "train Loss: 0.16013679030488748 Acc: 0.9508196721311475\n",
      "val Loss: 0.18491691759988374 Acc: 0.9411764705882353\n",
      "\n",
      "Training compete in 0.0m 14.700076580047607s\n",
      "Best val Acc: 0.954248366013072\n"
     ]
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, ohist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n",
      "train Loss: 0.7185551504619786 Acc: 0.4426229508196721\n",
      "val Loss: 0.6956208067781785 Acc: 0.45751633986928103\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n",
      "train Loss: 0.6852761008700387 Acc: 0.5778688524590164\n",
      "val Loss: 0.6626271987273022 Acc: 0.6601307189542484\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n",
      "train Loss: 0.6603062289660094 Acc: 0.5942622950819673\n",
      "val Loss: 0.6489538297154545 Acc: 0.5816993464052288\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n",
      "train Loss: 0.6203305486772881 Acc: 0.639344262295082\n",
      "val Loss: 0.6013184107986151 Acc: 0.673202614379085\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n",
      "train Loss: 0.5989709232674271 Acc: 0.6680327868852459\n",
      "val Loss: 0.5929347966231552 Acc: 0.6993464052287581\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n",
      "train Loss: 0.5821619336722327 Acc: 0.6557377049180327\n",
      "val Loss: 0.5804777059679717 Acc: 0.6928104575163399\n",
      "\n",
      "Epoch 6/14\n",
      "----------\n",
      "train Loss: 0.6114685896967278 Acc: 0.6270491803278688\n",
      "val Loss: 0.5674225290616354 Acc: 0.7189542483660131\n",
      "\n",
      "Epoch 7/14\n",
      "----------\n",
      "train Loss: 0.5681056575696977 Acc: 0.6680327868852459\n",
      "val Loss: 0.5602688086188696 Acc: 0.7189542483660131\n",
      "\n",
      "Epoch 8/14\n",
      "----------\n",
      "train Loss: 0.5701596453541615 Acc: 0.7090163934426229\n",
      "val Loss: 0.5554519264526616 Acc: 0.7450980392156863\n",
      "\n",
      "Epoch 9/14\n",
      "----------\n",
      "train Loss: 0.5476810380083615 Acc: 0.7254098360655737\n",
      "val Loss: 0.5805927063125411 Acc: 0.7189542483660131\n",
      "\n",
      "Epoch 10/14\n",
      "----------\n",
      "train Loss: 0.5508710468401674 Acc: 0.6926229508196722\n",
      "val Loss: 0.5859468777974447 Acc: 0.7058823529411765\n",
      "\n",
      "Epoch 11/14\n",
      "----------\n",
      "train Loss: 0.5344281519045595 Acc: 0.7172131147540983\n",
      "val Loss: 0.5640550851821899 Acc: 0.7058823529411765\n",
      "\n",
      "Epoch 12/14\n",
      "----------\n",
      "train Loss: 0.5125471890949812 Acc: 0.7295081967213115\n",
      "val Loss: 0.5665123891207128 Acc: 0.7058823529411765\n",
      "\n",
      "Epoch 13/14\n",
      "----------\n",
      "train Loss: 0.496260079204059 Acc: 0.7254098360655737\n",
      "val Loss: 0.5820710787586137 Acc: 0.7058823529411765\n",
      "\n",
      "Epoch 14/14\n",
      "----------\n",
      "train Loss: 0.49067981907578767 Acc: 0.7704918032786885\n",
      "val Loss: 0.5722863315756804 Acc: 0.7058823529411765\n",
      "\n",
      "Training compete in 0.0m 18.418847799301147s\n",
      "Best val Acc: 0.7450980392156863\n"
     ]
    }
   ],
   "source": [
    "# Initialize the non-pretrained version of the model used for this run\n",
    "scratch_model,_ = initialize_model(model_name, num_classes, feature_extract=False, use_pretrained=False)\n",
    "scratch_model = scratch_model.to(device)\n",
    "scratch_optimizer = optim.SGD(scratch_model.parameters(), lr=0.001, momentum=0.9)\n",
    "scratch_criterion = nn.CrossEntropyLoss()\n",
    "_,scratch_hist = train_model(scratch_model, dataloaders_dict, scratch_criterion, scratch_optimizer, num_epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/weh0KINFHnYuQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Plot the training curves of validation accuracy vs. number \n",
    "#  of training epochs for the transfer learning method and\n",
    "#  the model trained from scratch\n",
    "# ohist = []\n",
    "# shist = []\n",
    "\n",
    "# ohist = [h.cpu().numpy() for h in ohist]\n",
    "# shist = [h.cpu().numpy() for h in scratch_hist]\n",
    "\n",
    "plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "plt.plot(range(1,num_epochs+1),scratch_hist,label=\"Scratch\")\n",
    "plt.ylim((0,1.))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
