{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 语言模型（输入一个句子，输出这个句子产生的概率）\n",
    "\n",
    "目标：根据之前的单词预测下一个单词。\n",
    "\n",
    "\n",
    "学习目标\n",
    "- 学习语言模型，以及如何训练一个语言模型\n",
    "- 学习torchtext的基本使用方法\n",
    "    - 构建 vocabulary\n",
    "    - word to inde 和 index to word\n",
    "- 学习torch.nn的一些基本模型\n",
    "    - Linear\n",
    "    - RNN\n",
    "    - LSTM\n",
    "    - GRU\n",
    "- RNN的训练技巧\n",
    "    - Gradient Clipping\n",
    "- 如何保存和读取模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调用工程需要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch.nn as nn\n",
    "\n",
    "USE_CUDA=torch.cuda.is_available()\n",
    "device=torch.device('cuda' if USE_CUDA else 'cpu')\n",
    "\n",
    "#固定random seed\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if USE_CUDA:\n",
    "    torch.cudada.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义相关参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#一个bantch中有多少个句子\n",
    "BATCH_SIZE=32 \n",
    "#word embedding 的维度\n",
    "EMBEDDING_SIZE=100\n",
    "MAX_VOCAB_SIZE=50000\n",
    "SEQ_LENGTH=20\n",
    "#隐含层神经元个数\n",
    "HIDDEN_SIZE = 100\n",
    "NUM_EPOCHES=2\n",
    "learning_rate=0.001\n",
    "GRAD_CLIP=5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建vocabulary(单词表)\n",
    "- 安装[torchtext](https://github.com/pytorch/text)  (用于文本预处理)  \n",
    "    pip install torchtext  \n",
    "- 使用 torchtext 来创建vocabulary, 然后把数据读成batch的格式。请大家自行阅读README来学习torchtext。  \n",
    "- **注意变更**：  \n",
    "    torchtext.data.Field -> torchtext.legacy.data.Field  \n",
    "    torchtext.datasets.LanguageModelingDataset -> torchtext.legacy.datasets.LanguageModelingDataset  \n",
    "    torchtext.data.BPTTIterator -> torchtext.legacy.data.BPTTIterator  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用field预处理数据；利用LanguageMOdelingDataset class创建三个dataset\n",
    "继续使用text8数据集作为训练、验证和测试数据\n",
    "1. TorchText的一个重要概念是[Field](https://torchtext.readthedocs.io/en/latest/data.html#field)，其决定了数据会被如何处理  \n",
    "    我们使用TEXT这个field来处理文本数据  \n",
    "    我们的TEXT field有lower-Ture这个参数，故所有的单词都会被lowercase  \n",
    "    torchtext提供了LanguageModelingDataset这个class来帮助处理语言模型数据集  \n",
    "2. build_vocab可以根据我们提供的训练数据集来创建最高频单词的单词表，max_size帮助我们限定单词总量\n",
    "3. BPTTIterator可以连续地获得连贯的句子，[BPTT](https://zh.d2l.ai/chapter_recurrent-neural-networks/bptt.html): back propagation through time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Re_AC\\Desktop\\Pytorch\\myTorch\\3\\languageModelNoteBook\\text8 <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "#确定数据集路径\n",
    "script_path=os.path.abspath('__file__')\n",
    "dir_path=os.path.dirname(script_path)\n",
    "path=os.path.join(dir_path,'text8')\n",
    "print(path,type(path))\n",
    "\n",
    "#创建一个名为TEXT的Field\n",
    "#lower=True: 将所有单词lowercase\n",
    "TEXT=torchtext.legacy.data.Field(lower=True)\n",
    "#创建用于language modeling的train, val, test三个dataset\n",
    "#将data split\n",
    "train, val, test = torchtext.legacy.datasets.LanguageModelingDataset.splits(path=path, \n",
    "                                                                            train='text8.train.txt', \n",
    "                                                                            validation='text8.dev.txt', \n",
    "                                                                            test='text8.test.txt', \n",
    "                                                                            text_field=TEXT)\n",
    "# print(train)\n",
    "# print(dir(train))\n",
    "# print(train.examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建Vocabulary\n",
    "- 创建vocabulary(单词表)相当于__myTorch/2/wordEmbeddingNotebook/2.ipynb#数据预处理及相关操作__中创建vocab参数的过程\n",
    "- 具体流程是从dataset中取出出现频数最高的前MAX_BOCAB_SIZE个单词作为Vocabulary\n",
    "- 单词表单词个数为50002个而不是50000个，是因为TorchText为我们增加了两个特殊的token：  \n",
    "    \\< unk \\>: 表示未知的，不在单词表中的单词  \n",
    "    \\< pad \\>: 表示padding，当句子较短时，将\\< pad \\>添加进句子末尾补齐长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50002\n",
      "<class 'list'>\n",
      "['<unk>', '<pad>', 'the', 'of', 'and', 'one', 'in', 'a', 'to', 'zero']\n",
      "<class 'collections.defaultdict'>\n",
      "1259\n"
     ]
    }
   ],
   "source": [
    "#创建training dataset的vocabulary 单词数量为MAX_BOCAB_SIZE\n",
    "TEXT.build_vocab(train, max_size=MAX_VOCAB_SIZE)\n",
    "#注意单词个数是50002个，而不是MAX_BOCAB_SIZE指定的50000个\n",
    "\n",
    "#定义VOCAB_SIZE\n",
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "print(len(TEXT.vocab)) #vocabulary size\n",
    "\n",
    "#itos: index to string\n",
    "print(type(TEXT.vocab.itos))\n",
    "print(TEXT.vocab.itos[:10]) #注意<unk>和<pad>\n",
    "\n",
    "#stoi: string to index\n",
    "print(type(TEXT.vocab.stoi))\n",
    "print(TEXT.vocab.stoi['apple'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建batch(iterator)\n",
    "为dataset创建batch，每个batch包含BATCH_SIZE个句子  \n",
    "句子长度seq_length(=bptt_len) 其沿时间方向  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bptt_len:  Length of sequences for backpropagation through time.\n",
    "#此处也决定了batch中每个句子的长度\n",
    "#具体参考：https://zh.d2l.ai/chapter_recurrent-neural-networks/bptt.html\n",
    "#repeat=False: 过完一边dataset后就结束一次epoch\n",
    "train_iter, val_iter, test_iter=torchtext.legacy.data.BPTTIterator.splits(\n",
    "    (train, val, test), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    device=device, \n",
    "    bptt_len=SEQ_LENGTH, \n",
    "    repeat=False, \n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.legacy.data.batch.Batch of size 32]\n",
      "\t[.text]:[torch.LongTensor of size 20x32]\n",
      "\t[.target]:[torch.LongTensor of size 20x32]\n",
      "tensor([[ 5269,  6271,   417,     9,     6,   375,   317,  2278,     6,    21,\n",
      "            72,    54,   742,     2,  4434,   283,    23,   531,     0,     5,\n",
      "           463,  5850,    22,  8624,  1455,    68,    11,    66,     2,  5931,\n",
      "             3, 24395],\n",
      "        [ 3110,     6,   288,     2,  3047,     2,    25,   109,   261,    50,\n",
      "          6129,   892,     7, 24782,    25, 12713,    18,     5,   556,    10,\n",
      "             7,  4664,     5,    43,   163,     5,     9,     2,  1311,    57,\n",
      "           168,     6],\n",
      "        [   13,  3593,   458,  1259,    40,   375,    10,   550,     3, 19798,\n",
      "            21, 43004, 17114,     3,     2,     7,  2316,    10,   427,     5,\n",
      "          1185,   127,    48,   504,  2461, 14097,     9,   277,     3,    12,\n",
      "         27121,   314],\n",
      "        [    7,     4, 11211, 21733,    55,    19,    11,     4,  3278,  4858,\n",
      "           176,   119,   340, 12915,  8644,   381, 14236,     5,   882,    18,\n",
      "          8991,   416,    49,    27,     8,  1435,    18,     6,     2,    95,\n",
      "           497,  1853],\n",
      "        [  196,   105,  3693,  1416,   289,    78,    17, 31461,  1180,     3,\n",
      "           130,     5,     3,    19,   211,   139, 15273,    10,   334,  4110,\n",
      "            36,  4664, 18390,    90, 13578,     2,   312,    37,  6939,  1918,\n",
      "            25,    19],\n",
      "        [    3,   191,   894,    32,  5157,   332,    11,    41,   202, 42442,\n",
      "           176,  5510,     2,    21,   712,     3,  5734,    22,   260,    26,\n",
      "             2,    59,   241,   130,   349,  5703,  2425,    48,   282,    35,\n",
      "          3892,     7],\n",
      "        [ 3081,    66,    58,  1676,    21,  1040,    17, 11031,     3,   943,\n",
      "           119,   278,   632,    28,     3,    34,   443,    11,   638,     2,\n",
      "            65,  2379,    80,   378,   318,    88,    15,   544,   144,    42,\n",
      "         28432,   378],\n",
      "        [   47, 29094,  7591,  1416,     2,     4,    18,   126,   448,   804,\n",
      "            34,     2,   801,    19,  2199,   465,  5734,    36,   427, 16334,\n",
      "         23503,     0,     7,     3,    30,     6,     7,   231,    58,     8,\n",
      "           737,     3],\n",
      "        [   61,     3, 11211,    15, 15041,     2,  2034,    48,  2316,    64,\n",
      "          9953,  1052,     8,  2651,  1676,     3,  4246,     2,   882,     3,\n",
      "          1260,  2708,   115,   502, 37123,   576,    70,   728,  1442,    33,\n",
      "           172, 21841],\n",
      "        [  156,  3593,  4428,     2,  1398,  2555,     6,   135,  7335,  6329,\n",
      "          2853,   652,    31,     8,    60,  4142, 20354,    65,   334,  2341,\n",
      "          3204, 16371,     3,    30,     2,    13,     5,  1198,     4,  2310,\n",
      "          1148,     4],\n",
      "        [  129,    37, 17357, 12597,    76,    19,     2,     0,  2326,    43,\n",
      "           838,  2325,  2934,  5277,    27,    12,   263,     2, 36540,    29,\n",
      "            12, 26747,   172,   130, 17485,     2,   125,   156,    44, 20285,\n",
      "           715,     0],\n",
      "        [  746,    27,     6,   282,  2838,  9186,  3176,    15,   114,  2133,\n",
      "             3,   350,   486,    60,     7,  8607,     5,  1947,     0,     2,\n",
      "          5453,    11, 12832,  1132,   681,    16,     5,     2,  1519,     8,\n",
      "         19883,    87],\n",
      "        [  459,     2, 17655,     3,     2,   713,   212,     2,   953, 42442,\n",
      "          5137,  1270,    16,    41,   406,    20,    10,    19,     0,   246,\n",
      "            80,     9,     3,     3,  1455,     0,    17,   550,    93,    31,\n",
      "             3,     2],\n",
      "        [10588,  4859,    27,  3489,   676,    62,    36,  8433,   202,   215,\n",
      "            19,     8,   128,    11,     3,   600,    17,   188,  2543,   128,\n",
      "           327,     9,   550,   502,    15,     0,  1627,   550,  9705,  3923,\n",
      "           550,  8626],\n",
      "        [  137,  2320,   698,  1181,  8238,     2,     2,     3,     3,    38,\n",
      "            47,    34,    42,  2129,     5, 49089,    11,   221,     0,  1059,\n",
      "           198,    18,  2740,  1740,   139,   184,     6, 13879, 43117,   183,\n",
      "             4,    84],\n",
      "        [    2,  1067,  4063,    19,    56,   168,     5,   369,   448,   141,\n",
      "           253,    28,    93, 14375,    17,    16,   850,  1045,    19,  2341,\n",
      "           636,     2,    20,    31, 26958,   951,     2,    20,    13,    87,\n",
      "            20, 24395],\n",
      "        [25887,     3,  6574,   494,    13,  4307,    10,   723,   600,   218,\n",
      "            20,  1445,    55,    36,  1127,   750,     0,  1103,     7,    20,\n",
      "          5453,  4721,   168,   300,    49,  3548,  5914,     2,    45,  4283,\n",
      "           524,    19],\n",
      "        [    3,   842,   142,  2213,     2,     6,    10,     0,  1307, 14635,\n",
      "          7361,    98,  2495,     2,  4434,  2972,     5, 12834,   413,   838,\n",
      "            80,   345,  1097,   122,     8,    16,  4992,  3313,  4613,    13,\n",
      "             3,  2182],\n",
      "        [    2,    34,     2,     7,  2195,     5,    10,  4473,   980,     4,\n",
      "            51,  1888,  4159,  8769,  1224,    60,    24,    29,   541,   728,\n",
      "             2,   342,     6, 25375, 42905,     0,     3, 13994,  5355,     3,\n",
      "             5,    20],\n",
      "        [  102,  8292, 11211, 34884,   587,    10,  1836,   567,  4536,    46,\n",
      "         17902,     2,     6,   237,     4,    16,    10,     7,  3344,  5071,\n",
      "          2915,   588,     2,   232, 12301,     0,     7,    21,   140,   547,\n",
      "            10,  5765]])\n",
      "torch.Size([20, 32])\n",
      "anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english\n",
      "\n",
      "originated as a term of abuse first used against early working class radicals including the diggers of the english revolution\n"
     ]
    }
   ],
   "source": [
    "#测试+加深理解\n",
    "it=iter(train_iter)\n",
    "batch=next(it)\n",
    "print(batch)\n",
    "#20: 句子长度seq_length(=bptt_len) 其沿时间方向  32: batch_size\n",
    "# [torchtext.legacy.data.batch.Batch of size 32]\n",
    "# \t[.text]:[torch.LongTensor of size 20x32]\n",
    "# \t[.target]:[torch.LongTensor of size 20x32]\n",
    "\n",
    "#可以看到text为文件：text8.train.txt的内容\n",
    "#target与text相似，但从text中的下一个单词开始，比text多一个单词结束\n",
    "#输入dataset中的一个单词，target（输出）为dataset中的下一个单词\n",
    "#模型的目的是预测下一个单词是什么\n",
    "print(batch.text)\n",
    "print(batch.text.shape)\n",
    "print(' '.join(TEXT.vocab.itos[i] for i in batch.text[:,0].data))\n",
    "print()\n",
    "print(' '.join(TEXT.vocab.itos[i] for i in batch.target[:,0].data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0\n",
      "revolution and the sans <unk> of the french revolution whilst the term is still used in a pejorative way to\n",
      "\n",
      "and the sans <unk> of the french revolution whilst the term is still used in a pejorative way to describe\n",
      "\n",
      "1\n",
      "describe any act that used violent means to destroy the organization of society it has also been taken up as\n",
      "\n",
      "any act that used violent means to destroy the organization of society it has also been taken up as a\n",
      "\n",
      "2\n",
      "a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king\n",
      "\n",
      "positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism\n",
      "\n",
      "3\n",
      "anarchism as a political philosophy is the belief that rulers are unnecessary and should be abolished although there are differing\n",
      "\n",
      "as a political philosophy is the belief that rulers are unnecessary and should be abolished although there are differing interpretations\n",
      "\n",
      "4\n",
      "interpretations of what this means anarchism also refers to related social movements that advocate the elimination of authoritarian institutions particularly\n",
      "\n",
      "of what this means anarchism also refers to related social movements that advocate the elimination of authoritarian institutions particularly the\n"
     ]
    }
   ],
   "source": [
    "#多拿几个train_iter中的batch，看看text和target中的内容\n",
    "for i in range(5):\n",
    "    batch=next(it)\n",
    "    print()\n",
    "    print(i)\n",
    "    print(' '.join(TEXT.vocab.itos[i] for i in batch.text[:,0].data))\n",
    "    print()\n",
    "    print(' '.join(TEXT.vocab.itos[i] for i in batch.target[:,0].data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型（简单的）\n",
    "- 继承nn.Module\n",
    "- \\_\\_init\\_\\_函数\n",
    "- forward函数\n",
    "- 其余可以根据模型需要定义相关函数  \n",
    "\n",
    "[**nn.Embedding及rnn输入**](https://www.jianshu.com/p/63e7acc5e890)\n",
    "\n",
    "**PyTorch处理RNN时默认第一个维度为sequence length，第二个维度为batch_size** \n",
    "\n",
    "\n",
    "每个batch：  \n",
    "    第一次输入LSTM的是batch_size个'句子'的第一个单词的embedding  \n",
    "    第二次输入LSTM的是这batch_size个'句子'的第二个单词的embedding  \n",
    "    。。。  \n",
    "    第seq_length次输入LSTM的是这batch_size个'句子'的第seq_length个单词的embedding  \n",
    "    至此根据这bptt_len即seq_length次输出计算loss和bptt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义一个简单的RNN （一层）\n",
    "class RNNModel(nn.Module):\n",
    "    #定义需要参数\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size=hidden_size\n",
    "        \n",
    "        #embedding层\n",
    "        self.embed=nn.Embedding(vocab_size, embed_size) # W大小：(50002, 650) \n",
    "        #LSTM层\n",
    "        #https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "        self.lstm=nn.LSTM(embed_size, hidden_size)\n",
    "        # batch_first=True: 将lstm第一个维度改为batch_size\n",
    "        # self.lstm=nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        # 将LSTM的结果decode为一个vocab_size维的向量，以确定预测的单词\n",
    "        self.linear=nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    #定义网络架构\n",
    "    def forward(self, input_text, hidden):\n",
    "        #forward pass\n",
    "        #input_text: seq_length * batch_size(32)\n",
    "        emb= self.embed(input_text) # seq_length * batch_size * embed_size\n",
    "        #embedding传入LSTM\n",
    "        #hidden: hidden state & cell state 两者形状相同\n",
    "        output, hidden = self.lstm(emb, hidden)\n",
    "        # output: seq_length * batch_size * hidden_size\n",
    "        # hidden: (1*batch_size*hiddensize, 1*batch_size*hidden_size) 1: LSTM层数为1, hidden state 参数同LSTMdancing的输出的形状相同\n",
    "        output_reshape=output.view(-1, output.shape[2]) #reshape output: (seq_length * batch_size) * hidden_size\n",
    "        out_vocab=self.linear(output_reshape) # (seq_length * batch_size) * vocab_size\n",
    "        #将out_vocab变回原来的形状\n",
    "        out_vocab=out_vocab.view(output.shape[0], output.shape[1], out_vocab.shape[-1]) # (seq_length * batch_size * vocab_size)\n",
    "        \n",
    "        return out_vocab, hidden\n",
    "    \n",
    "    #初始化hidden state 和 cell state\n",
    "    def init_hidden(self, batch_size, requires_grad=True):\n",
    "        #从model中随便选取一组parameters 为了方便，直接用next\n",
    "        #此步操作原因见下一步\n",
    "        weight= next(self.parameters())\n",
    "        #使用0矩阵初始化hidden state和cell state\n",
    "        #为了保证创建tensor与model中其他tensor有相同的torch.dtype 和 torch.device， 使用new_zeros函数\n",
    "        hidden_state=weight.new_zeros((1, batch_size, self.hidden_size), requires_grad= requires_grad)\n",
    "        cell_state=weight.new_zeros((1, batch_size, self.hidden_size), requires_grad= requires_grad)\n",
    "        \n",
    "        return (hidden_state, cell_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNModel(\n",
      "  (embed): Embedding(50002, 20)\n",
      "  (lstm): LSTM(20, 100)\n",
      "  (linear): Linear(in_features=100, out_features=50002, bias=True)\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([[-1.5256, -0.7502, -0.6540,  ..., -1.1608,  0.6995,  0.1991],\n",
      "        [ 0.8657,  0.2444, -0.6629,  ...,  0.0457,  0.1530, -0.4757],\n",
      "        [-0.1110,  0.2927, -0.1578,  ...,  0.9386, -0.1860, -0.6446],\n",
      "        ...,\n",
      "        [-0.1827,  0.1614, -0.6383,  ...,  0.2086,  0.8639, -0.8471],\n",
      "        [ 1.5322, -0.3728,  0.5348,  ..., -1.6007,  0.6433,  0.7884],\n",
      "        [-0.2278,  0.3506,  0.2607,  ..., -2.5110, -0.7777,  0.8388]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model=RNNModel(vocab_size=VOCAB_SIZE, embed_size=EMBEDDING_SIZE, hidden_size=HIDDEN_SIZE)\n",
    "if USE_CUDA:\n",
    "    model=model.to(device)\n",
    "\n",
    "print(model)\n",
    "print(next(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型及保存模型\n",
    "- 模型一般需要训练若干个epoch\n",
    "- 每个epoch我们都把所有的数据分成若干个batch\n",
    "- 把每个batch的输入和输出都包装成cuda tensor\n",
    "- forward pass，通过输入的句子预测每个单词的下一个单词\n",
    "- 用模型的预测和正确的下一个单词计算cross entropy loss\n",
    "- backward pass\n",
    "- gradient clipping，防止梯度爆炸\n",
    "- 更新模型参数\n",
    "- 清空模型当前gradient\n",
    "- 每隔一定的iteration输出模型在当前iteration的loss，以及在验证集上做模型的评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hidden state/cell Tensor在Torch的graph中作为一个节点，其与W类似，与历史的hidden state/cell都有关系\n",
    "#由于hidden state/cell 一直往下传递，计算图会非常大非常深，最终可能会导致内存爆炸\n",
    "#所以利用detach将hidden state/cell同之前的hidden state/cell分离\n",
    "#这样backpropagation会从分离的部分重新开始\n",
    "\n",
    "#detach: https://pytorch.org/docs/stable/generated/torch.Tensor.detach.html\n",
    "#Returns a new Tensor, detached from the current graph.\n",
    "#The result will never require gradient.\n",
    "\n",
    "def repackage_hidden(hidden):\n",
    "    #如果hidden是Tensor\n",
    "    \n",
    "    # isinstance(object, classinfo)\n",
    "    # 如果对象的类型与参数二的类型（classinfo）相同则返回 True，否则返回 False\n",
    "    if isinstance(hidden, torch.Tensor):\n",
    "        return hidden.detach()\n",
    "    #否则是(hidden_state, cell_state)元组\n",
    "    #递归调用，将两者截断后重新组成元组\n",
    "    else:\n",
    "        return tuple(repackage_hidden(i) for i in hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义loss fun和optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0 :  10.811491012573242\n",
      "loss 100 :  7.245739936828613\n",
      "loss 200 :  7.6914544105529785\n",
      "loss 300 :  7.333763122558594\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-22285615e0d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m#output: (seq_length * batch_size，vocab_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m#garget.view: (seq_length * batch_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m#backward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venv_pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venv_pytorch\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1046\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m-> 1048\u001b[1;33m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[0;32m   1049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venv_pytorch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2691\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2692\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2693\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2694\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venv_pytorch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1670\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"log_softmax\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1671\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1672\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1673\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1674\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHES):\n",
    "    #Sets the module in training mode.\n",
    "    model.train()\n",
    "    #将train_iter转化为迭代器\n",
    "    it=iter(train_iter)\n",
    "    #初始化hidden state\n",
    "    hidden= model.init_hidden(BATCH_SIZE)\n",
    "    #enumerate: 为迭代器每次迭代添加序号\n",
    "    for i, batch in enumerate(it):\n",
    "        data, target = batch.text, batch.target #已经在cuda上了，不需要进行设备转换: print(batch.text)\n",
    "        \n",
    "        #在每个batch调用hidden之前，将hidden与其之前的历史分离\n",
    "        #保证虽然利用了之前的hidden，但是bptt只在此次batch中进行\n",
    "        hidden=repackage_hidden(hidden)\n",
    "        \n",
    "        #在语言模型中， 训练集中的前一个句子与后一个句子是相连的\n",
    "        #所以下一个batch/iteration/下一个backpropagationThroughTime的过程仍然可以用上一次的hidden state\n",
    "        output, hidden =model(data, hidden)\n",
    "        \n",
    "        #output形状：(seq_length， batch_size， vocab_size)\n",
    "        #为了使用crossentropy计算loss，需要对output reshape为(seq_length * batch_size，vocab_size)\n",
    "        output=output.reshape(-1, VOCAB_SIZE)\n",
    "        \n",
    "        #计算loss\n",
    "        #将target也reshape成vector\n",
    "        #注意CrossEntropyLoss包含了LogSoftmax\n",
    "        #output: (seq_length * batch_size，vocab_size)\n",
    "        #garget.view: (seq_length * batch_size)\n",
    "        loss=loss_fn(output, target.view(-1))\n",
    "        \n",
    "        #backward\n",
    "        loss.backward()\n",
    "        \n",
    "        #将parameters clip，防止vanishing gradients and exploding gradients.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "        \n",
    "        #更新网络参数\n",
    "        optimizer.step()\n",
    "        \n",
    "        #清零gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #每100次输出loss\n",
    "        if i%100==0:\n",
    "            print('loss', i, ': ', loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pytorch",
   "language": "python",
   "name": "venv_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
