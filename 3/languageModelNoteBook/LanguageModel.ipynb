{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 语言模型（输入一个句子，输出这个句子产生的概率）\n",
    "\n",
    "目标：根据之前的单词预测下一个单词。\n",
    "\n",
    "\n",
    "学习目标\n",
    "- 学习语言模型，以及如何训练一个语言模型\n",
    "- 学习torchtext的基本使用方法\n",
    "    - 构建 vocabulary\n",
    "    - word to inde 和 index to word\n",
    "- 学习torch.nn的一些基本模型\n",
    "    - Linear\n",
    "    - RNN\n",
    "    - LSTM\n",
    "    - GRU\n",
    "- RNN的训练技巧\n",
    "    - Gradient Clipping\n",
    "- 如何保存和读取模型  \n",
    "- 更新learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调用工程需要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch.nn as nn\n",
    "\n",
    "USE_CUDA=torch.cuda.is_available()\n",
    "print(USE_CUDA)\n",
    "device=torch.device('cuda' if USE_CUDA else 'cpu')\n",
    "\n",
    "#固定random seed\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义相关参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#一个bantch中有多少个句子\n",
    "BATCH_SIZE=32 \n",
    "#word embedding 的维度\n",
    "EMBEDDING_SIZE=100\n",
    "MAX_VOCAB_SIZE=50000\n",
    "SEQ_LENGTH=20\n",
    "#隐含层神经元个数\n",
    "HIDDEN_SIZE = 100\n",
    "NUM_EPOCHES=2\n",
    "learning_rate=0.001\n",
    "GRAD_CLIP=5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建vocabulary(单词表)\n",
    "- 安装[torchtext](https://github.com/pytorch/text)  (用于文本预处理)  \n",
    "    pip install torchtext  \n",
    "- 使用 torchtext 来创建vocabulary, 然后把数据读成batch的格式。请大家自行阅读README来学习torchtext。 \n",
    "- 注意torchtext与torch的版本是否匹配，不匹配可能会出现torch版本被新版本torch_cpu替代的情况\n",
    "\n",
    "- **注意变更**：  \n",
    "    torchtext.data.Field -> torchtext.legacy.data.Field  \n",
    "    torchtext.datasets.LanguageModelingDataset -> torchtext.legacy.datasets.LanguageModelingDataset  \n",
    "    torchtext.data.BPTTIterator -> torchtext.legacy.data.BPTTIterator  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用field预处理数据；利用LanguageMOdelingDataset class创建三个dataset\n",
    "继续使用text8数据集作为训练、验证和测试数据\n",
    "1. TorchText的一个重要概念是[Field](https://torchtext.readthedocs.io/en/latest/data.html#field)，其决定了数据会被如何处理  \n",
    "    我们使用TEXT这个field来处理文本数据  \n",
    "    我们的TEXT field有lower-Ture这个参数，故所有的单词都会被lowercase  \n",
    "    torchtext提供了LanguageModelingDataset这个class来帮助处理语言模型数据集  \n",
    "2. build_vocab可以根据我们提供的训练数据集来创建最高频单词的单词表，max_size帮助我们限定单词总量\n",
    "3. BPTTIterator可以连续地获得连贯的句子，[BPTT](https://zh.d2l.ai/chapter_recurrent-neural-networks/bptt.html): back propagation through time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Re_AC\\Desktop\\Pytorch\\myTorch\\3\\languageModelNoteBook\\text8 <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "#确定数据集路径\n",
    "script_path=os.path.abspath('__file__')\n",
    "dir_path=os.path.dirname(script_path)\n",
    "path=os.path.join(dir_path,'text8')\n",
    "print(path,type(path))\n",
    "\n",
    "#创建一个名为TEXT的Field\n",
    "#lower=True: 将所有单词lowercase\n",
    "TEXT=torchtext.legacy.data.Field(lower=True)\n",
    "#创建用于language modeling的train, val, test三个dataset\n",
    "#将data split\n",
    "train, val, test = torchtext.legacy.datasets.LanguageModelingDataset.splits(path=path, \n",
    "                                                                            train='text8.train.txt', \n",
    "                                                                            validation='text8.dev.txt', \n",
    "                                                                            test='text8.test.txt', \n",
    "                                                                            text_field=TEXT)\n",
    "# print(train)\n",
    "# print(dir(train))\n",
    "# print(train.examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建Vocabulary\n",
    "- 创建vocabulary(单词表)相当于__myTorch/2/wordEmbeddingNotebook/2.ipynb#数据预处理及相关操作__中创建vocab参数的过程\n",
    "- 具体流程是从dataset中取出出现频数最高的前MAX_BOCAB_SIZE个单词作为Vocabulary\n",
    "- 单词表单词个数为50002个而不是50000个，是因为TorchText为我们增加了两个特殊的token：  \n",
    "    \\< unk \\>: 表示未知的，不在单词表中的单词  \n",
    "    \\< pad \\>: 表示padding，当句子较短时，将\\< pad \\>添加进句子末尾补齐长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50002\n",
      "<class 'list'>\n",
      "['<unk>', '<pad>', 'the', 'of', 'and', 'one', 'in', 'a', 'to', 'zero']\n",
      "<class 'collections.defaultdict'>\n",
      "1259\n"
     ]
    }
   ],
   "source": [
    "#创建training dataset的vocabulary 单词数量为MAX_BOCAB_SIZE\n",
    "TEXT.build_vocab(train, max_size=MAX_VOCAB_SIZE)\n",
    "#注意单词个数是50002个，而不是MAX_BOCAB_SIZE指定的50000个\n",
    "\n",
    "#定义VOCAB_SIZE\n",
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "print(len(TEXT.vocab)) #vocabulary size\n",
    "\n",
    "#itos: index to string\n",
    "print(type(TEXT.vocab.itos))\n",
    "print(TEXT.vocab.itos[:10]) #注意<unk>和<pad>\n",
    "\n",
    "#stoi: string to index\n",
    "print(type(TEXT.vocab.stoi))\n",
    "print(TEXT.vocab.stoi['apple'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建batch(iterator)\n",
    "为dataset创建batch，每个batch包含BATCH_SIZE个句子  \n",
    "句子长度seq_length(=bptt_len) 其沿时间方向  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bptt_len:  Length of sequences for backpropagation through time.\n",
    "#此处也决定了batch中每个句子的长度\n",
    "#具体参考：https://zh.d2l.ai/chapter_recurrent-neural-networks/bptt.html\n",
    "#repeat=False: 过完一边dataset后就结束一次epoch\n",
    "train_iter, val_iter, test_iter=torchtext.legacy.data.BPTTIterator.splits(\n",
    "    (train, val, test), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    device=device, \n",
    "    bptt_len=SEQ_LENGTH, \n",
    "    repeat=False, \n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.legacy.data.batch.Batch of size 32]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 20x32 (GPU 0)]\n",
      "\t[.target]:[torch.cuda.LongTensor of size 20x32 (GPU 0)]\n",
      "tensor([[ 5269,  6271,   417,     9,     6,   375,   317,  2278,     6,    21,\n",
      "            72,    54,   742,     2,  4434,   283,    23,   531,     0,     5,\n",
      "           463,  5850,    22,  8624,  1455,    68,    11,    66,     2,  5931,\n",
      "             3, 24395],\n",
      "        [ 3110,     6,   288,     2,  3047,     2,    25,   109,   261,    50,\n",
      "          6129,   892,     7, 24782,    25, 12713,    18,     5,   556,    10,\n",
      "             7,  4664,     5,    43,   163,     5,     9,     2,  1311,    57,\n",
      "           168,     6],\n",
      "        [   13,  3593,   458,  1259,    40,   375,    10,   550,     3, 19798,\n",
      "            21, 43004, 17114,     3,     2,     7,  2316,    10,   427,     5,\n",
      "          1185,   127,    48,   504,  2461, 14097,     9,   277,     3,    12,\n",
      "         27121,   314],\n",
      "        [    7,     4, 11211, 21733,    55,    19,    11,     4,  3278,  4858,\n",
      "           176,   119,   340, 12915,  8644,   381, 14236,     5,   882,    18,\n",
      "          8991,   416,    49,    27,     8,  1435,    18,     6,     2,    95,\n",
      "           497,  1853],\n",
      "        [  196,   105,  3693,  1416,   289,    78,    17, 31461,  1180,     3,\n",
      "           130,     5,     3,    19,   211,   139, 15273,    10,   334,  4110,\n",
      "            36,  4664, 18390,    90, 13578,     2,   312,    37,  6939,  1918,\n",
      "            25,    19],\n",
      "        [    3,   191,   894,    32,  5157,   332,    11,    41,   202, 42442,\n",
      "           176,  5510,     2,    21,   712,     3,  5734,    22,   260,    26,\n",
      "             2,    59,   241,   130,   349,  5703,  2425,    48,   282,    35,\n",
      "          3892,     7],\n",
      "        [ 3081,    66,    58,  1676,    21,  1040,    17, 11031,     3,   943,\n",
      "           119,   278,   632,    28,     3,    34,   443,    11,   638,     2,\n",
      "            65,  2379,    80,   378,   318,    88,    15,   544,   144,    42,\n",
      "         28432,   378],\n",
      "        [   47, 29094,  7591,  1416,     2,     4,    18,   126,   448,   804,\n",
      "            34,     2,   801,    19,  2199,   465,  5734,    36,   427, 16334,\n",
      "         23503,     0,     7,     3,    30,     6,     7,   231,    58,     8,\n",
      "           737,     3],\n",
      "        [   61,     3, 11211,    15, 15041,     2,  2034,    48,  2316,    64,\n",
      "          9953,  1052,     8,  2651,  1676,     3,  4246,     2,   882,     3,\n",
      "          1260,  2708,   115,   502, 37123,   576,    70,   728,  1442,    33,\n",
      "           172, 21841],\n",
      "        [  156,  3593,  4428,     2,  1398,  2555,     6,   135,  7335,  6329,\n",
      "          2853,   652,    31,     8,    60,  4142, 20354,    65,   334,  2341,\n",
      "          3204, 16371,     3,    30,     2,    13,     5,  1198,     4,  2310,\n",
      "          1148,     4],\n",
      "        [  129,    37, 17357, 12597,    76,    19,     2,     0,  2326,    43,\n",
      "           838,  2325,  2934,  5277,    27,    12,   263,     2, 36540,    29,\n",
      "            12, 26747,   172,   130, 17485,     2,   125,   156,    44, 20285,\n",
      "           715,     0],\n",
      "        [  746,    27,     6,   282,  2838,  9186,  3176,    15,   114,  2133,\n",
      "             3,   350,   486,    60,     7,  8607,     5,  1947,     0,     2,\n",
      "          5453,    11, 12832,  1132,   681,    16,     5,     2,  1519,     8,\n",
      "         19883,    87],\n",
      "        [  459,     2, 17655,     3,     2,   713,   212,     2,   953, 42442,\n",
      "          5137,  1270,    16,    41,   406,    20,    10,    19,     0,   246,\n",
      "            80,     9,     3,     3,  1455,     0,    17,   550,    93,    31,\n",
      "             3,     2],\n",
      "        [10588,  4859,    27,  3489,   676,    62,    36,  8433,   202,   215,\n",
      "            19,     8,   128,    11,     3,   600,    17,   188,  2543,   128,\n",
      "           327,     9,   550,   502,    15,     0,  1627,   550,  9705,  3923,\n",
      "           550,  8626],\n",
      "        [  137,  2320,   698,  1181,  8238,     2,     2,     3,     3,    38,\n",
      "            47,    34,    42,  2129,     5, 49089,    11,   221,     0,  1059,\n",
      "           198,    18,  2740,  1740,   139,   184,     6, 13879, 43117,   183,\n",
      "             4,    84],\n",
      "        [    2,  1067,  4063,    19,    56,   168,     5,   369,   448,   141,\n",
      "           253,    28,    93, 14375,    17,    16,   850,  1045,    19,  2341,\n",
      "           636,     2,    20,    31, 26958,   951,     2,    20,    13,    87,\n",
      "            20, 24395],\n",
      "        [25887,     3,  6574,   494,    13,  4307,    10,   723,   600,   218,\n",
      "            20,  1445,    55,    36,  1127,   750,     0,  1103,     7,    20,\n",
      "          5453,  4721,   168,   300,    49,  3548,  5914,     2,    45,  4283,\n",
      "           524,    19],\n",
      "        [    3,   842,   142,  2213,     2,     6,    10,     0,  1307, 14635,\n",
      "          7361,    98,  2495,     2,  4434,  2972,     5, 12834,   413,   838,\n",
      "            80,   345,  1097,   122,     8,    16,  4992,  3313,  4613,    13,\n",
      "             3,  2182],\n",
      "        [    2,    34,     2,     7,  2195,     5,    10,  4473,   980,     4,\n",
      "            51,  1888,  4159,  8769,  1224,    60,    24,    29,   541,   728,\n",
      "             2,   342,     6, 25375, 42905,     0,     3, 13994,  5355,     3,\n",
      "             5,    20],\n",
      "        [  102,  8292, 11211, 34884,   587,    10,  1836,   567,  4536,    46,\n",
      "         17902,     2,     6,   237,     4,    16,    10,     7,  3344,  5071,\n",
      "          2915,   588,     2,   232, 12301,     0,     7,    21,   140,   547,\n",
      "            10,  5765]], device='cuda:0')\n",
      "torch.Size([20, 32])\n",
      "anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english\n",
      "\n",
      "originated as a term of abuse first used against early working class radicals including the diggers of the english revolution\n"
     ]
    }
   ],
   "source": [
    "#测试+加深理解\n",
    "it=iter(train_iter)\n",
    "batch=next(it)\n",
    "print(batch)\n",
    "#20: 句子长度seq_length(=bptt_len) 其沿时间方向  32: batch_size\n",
    "# [torchtext.legacy.data.batch.Batch of size 32]\n",
    "# \t[.text]:[torch.LongTensor of size 20x32]\n",
    "# \t[.target]:[torch.LongTensor of size 20x32]\n",
    "\n",
    "#可以看到text为文件：text8.train.txt的内容\n",
    "#target与text相似，但从text中的下一个单词开始，比text多一个单词结束\n",
    "#输入dataset中的一个单词，target（输出）为dataset中的下一个单词\n",
    "#模型的目的是预测下一个单词是什么\n",
    "print(batch.text)\n",
    "print(batch.text.shape)\n",
    "print(' '.join(TEXT.vocab.itos[i] for i in batch.text[:,0].data))\n",
    "print()\n",
    "print(' '.join(TEXT.vocab.itos[i] for i in batch.target[:,0].data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0\n",
      "revolution and the sans <unk> of the french revolution whilst the term is still used in a pejorative way to\n",
      "\n",
      "and the sans <unk> of the french revolution whilst the term is still used in a pejorative way to describe\n",
      "\n",
      "1\n",
      "describe any act that used violent means to destroy the organization of society it has also been taken up as\n",
      "\n",
      "any act that used violent means to destroy the organization of society it has also been taken up as a\n",
      "\n",
      "2\n",
      "a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king\n",
      "\n",
      "positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism\n",
      "\n",
      "3\n",
      "anarchism as a political philosophy is the belief that rulers are unnecessary and should be abolished although there are differing\n",
      "\n",
      "as a political philosophy is the belief that rulers are unnecessary and should be abolished although there are differing interpretations\n",
      "\n",
      "4\n",
      "interpretations of what this means anarchism also refers to related social movements that advocate the elimination of authoritarian institutions particularly\n",
      "\n",
      "of what this means anarchism also refers to related social movements that advocate the elimination of authoritarian institutions particularly the\n"
     ]
    }
   ],
   "source": [
    "#多拿几个train_iter中的batch，看看text和target中的内容\n",
    "for i in range(5):\n",
    "    batch=next(it)\n",
    "    print()\n",
    "    print(i)\n",
    "    print(' '.join(TEXT.vocab.itos[i] for i in batch.text[:,0].data))\n",
    "    print()\n",
    "    print(' '.join(TEXT.vocab.itos[i] for i in batch.target[:,0].data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型（简单的）\n",
    "- 继承nn.Module\n",
    "- \\_\\_init\\_\\_函数\n",
    "- forward函数\n",
    "- 其余可以根据模型需要定义相关函数  \n",
    "\n",
    "[**nn.Embedding及rnn输入**](https://www.jianshu.com/p/63e7acc5e890)\n",
    "\n",
    "**PyTorch处理RNN时默认第一个维度为sequence length，第二个维度为batch_size** \n",
    "\n",
    "\n",
    "每个batch：  \n",
    "    第一次输入LSTM的是batch_size个'句子'的第一个单词的embedding  \n",
    "    第二次输入LSTM的是这batch_size个'句子'的第二个单词的embedding  \n",
    "    。。。  \n",
    "    第seq_length次输入LSTM的是这batch_size个'句子'的第seq_length个单词的embedding  \n",
    "    至此根据这bptt_len即seq_length次输出计算loss和bptt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义一个简单的RNN （一层）\n",
    "class RNNModel(nn.Module):\n",
    "    #定义需要参数\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size=hidden_size\n",
    "        \n",
    "        #embedding层\n",
    "        self.embed=nn.Embedding(vocab_size, embed_size) # W大小：(50002, 650) \n",
    "        #LSTM层\n",
    "        #https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "        self.lstm=nn.LSTM(embed_size, hidden_size)\n",
    "        # batch_first=True: 将lstm第一个维度改为batch_size\n",
    "        # self.lstm=nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        # 将LSTM的结果decode为一个vocab_size维的向量，以确定预测的单词\n",
    "        self.linear=nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    #定义网络架构\n",
    "    def forward(self, input_text, hidden):\n",
    "        #forward pass\n",
    "        #input_text: seq_length * batch_size(32)\n",
    "        emb= self.embed(input_text) # seq_length * batch_size * embed_size\n",
    "        #embedding传入LSTM\n",
    "        #hidden: hidden state & cell state 两者形状相同\n",
    "        output, hidden = self.lstm(emb, hidden)\n",
    "        # output: seq_length * batch_size * hidden_size\n",
    "        # hidden: (1*batch_size*hiddensize, 1*batch_size*hidden_size) 1: LSTM层数为1, hidden state 参数同LSTMdancing的输出的形状相同\n",
    "        output_reshape=output.view(-1, output.shape[2]) #reshape output: (seq_length * batch_size) * hidden_size\n",
    "        out_vocab=self.linear(output_reshape) # (seq_length * batch_size) * vocab_size\n",
    "        #将out_vocab变回原来的形状\n",
    "        out_vocab=out_vocab.view(output.shape[0], output.shape[1], out_vocab.shape[-1]) # (seq_length * batch_size * vocab_size)\n",
    "        \n",
    "        return out_vocab, hidden\n",
    "    \n",
    "    #初始化hidden state 和 cell state\n",
    "    def init_hidden(self, batch_size, requires_grad=True):\n",
    "        #从model中随便选取一组parameters 为了方便，直接用next\n",
    "        #此步操作原因见下一步\n",
    "        weight= next(self.parameters())\n",
    "        #使用0矩阵初始化hidden state和cell state\n",
    "        #为了保证创建tensor与model中其他tensor有相同的torch.dtype 和 torch.device， 使用new_zeros函数\n",
    "        hidden_state=weight.new_zeros((1, batch_size, self.hidden_size), requires_grad= requires_grad)\n",
    "        cell_state=weight.new_zeros((1, batch_size, self.hidden_size), requires_grad= requires_grad)\n",
    "        \n",
    "        return (hidden_state, cell_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNModel(\n",
      "  (embed): Embedding(50002, 100)\n",
      "  (lstm): LSTM(100, 100)\n",
      "  (linear): Linear(in_features=100, out_features=50002, bias=True)\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([[-1.5256, -0.7502, -0.6540,  ...,  1.1899,  0.8165, -0.9135],\n",
      "        [ 1.3851, -0.8138, -0.9276,  ..., -1.8475, -2.9167, -0.5673],\n",
      "        [-0.5413,  0.8952, -0.8825,  ..., -0.0586,  1.1788,  0.6222],\n",
      "        ...,\n",
      "        [ 0.6637,  0.4019,  1.0508,  ..., -1.6378,  0.6289,  0.1546],\n",
      "        [ 2.7030,  1.1254,  1.1153,  ...,  1.6220,  0.7710, -0.3384],\n",
      "        [ 0.3367, -0.3162, -0.1132,  ...,  0.1047,  1.5384, -0.7781]],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model=RNNModel(vocab_size=VOCAB_SIZE, embed_size=EMBEDDING_SIZE, hidden_size=HIDDEN_SIZE)\n",
    "if USE_CUDA:\n",
    "    model=model.to(device)\n",
    "\n",
    "print(model)\n",
    "print(next(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型及保存模型\n",
    "- 模型一般需要训练若干个epoch\n",
    "- 每个epoch我们都把所有的数据分成若干个batch\n",
    "- 把每个batch的输入和输出都包装成cuda tensor\n",
    "- forward pass，通过输入的句子预测每个单词的下一个单词\n",
    "- 用模型的预测和正确的下一个单词计算cross entropy loss\n",
    "- backward pass\n",
    "- gradient clipping，防止梯度爆炸\n",
    "- 更新模型参数\n",
    "- 清空模型当前gradient\n",
    "- 每隔一定的iteration输出模型在当前iteration的loss，以及在验证集上做模型的评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hidden state/cell Tensor在Torch的graph中作为一个节点，其与W类似，与历史的hidden state/cell都有关系\n",
    "#由于hidden state/cell 一直往下传递，计算图会非常大非常深，最终可能会导致内存爆炸\n",
    "#所以利用detach将hidden state/cell同之前的hidden state/cell分离\n",
    "#这样backpropagation会从分离的部分重新开始\n",
    "\n",
    "#detach: https://pytorch.org/docs/stable/generated/torch.Tensor.detach.html\n",
    "#Returns a new Tensor, detached from the current graph.\n",
    "#The result will never require gradient.\n",
    "\n",
    "def repackage_hidden(hidden):\n",
    "    #如果hidden是Tensor\n",
    "    \n",
    "    # isinstance(object, classinfo)\n",
    "    # 如果对象的类型与参数二的类型（classinfo）相同则返回 True，否则返回 False\n",
    "    if isinstance(hidden, torch.Tensor):\n",
    "        return hidden.detach()\n",
    "    #否则是(hidden_state, cell_state)元组\n",
    "    #递归调用，将两者截断后重新组成元组\n",
    "    else:\n",
    "        return tuple(repackage_hidden(i) for i in hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义loss fun和optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "#每call一次该函数，将learning下降一点\n",
    "#0.5： 将learning rate下降到原来的50%\n",
    "scheduler=torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义evaluate函数，用于验证集对模型的评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用于保存在验证集上评估的结果\n",
    "#和训练模型更新参数前的操作基本类似\n",
    "val_losses=[]\n",
    "\n",
    "def evaluate(model, input_data):\n",
    "    #Sets the module in evaluation mode.\n",
    "    model.eval()\n",
    "    #保存total loss\n",
    "    total_loss=0.\n",
    "    #保存一共预测的单词数\n",
    "    total_count=0\n",
    "    #将data转化为迭代器\n",
    "    it=iter(input_data)\n",
    "    \n",
    "    #因为是做预测，所以所有参数都不应该有gradient\n",
    "    #临时让所有参数不计算grad\n",
    "    with torch.no_grad():\n",
    "        #初始化hidden state\n",
    "        hidden= model.init_hidden(BATCH_SIZE, requires_grad=False)\n",
    "        #enumerate: 为迭代器每次迭代添加序号\n",
    "        for i, batch in enumerate(it):\n",
    "            data, target = batch.text, batch.target #已经在cuda上了，不需要进行设备转换: print(batch.text)\n",
    "\n",
    "            #在每个batch调用hidden之前，将hidden与其之前的历史分离\n",
    "            #保证虽然利用了之前的hidden，但是bptt只在此次batch中进行\n",
    "            hidden=repackage_hidden(hidden)\n",
    "\n",
    "            #在语言模型中， 训练集中的前一个句子与后一个句子是相连的\n",
    "            #所以下一个batch/iteration/下一个backpropagationThroughTime的过程仍然可以用上一次的hidden state\n",
    "            output, hidden =model(data, hidden)\n",
    "\n",
    "            #output形状：(seq_length， batch_size， vocab_size)\n",
    "            #为了使用crossentropy计算loss，需要对output reshape为(seq_length * batch_size，vocab_size)\n",
    "            output=output.reshape(-1, VOCAB_SIZE)\n",
    "\n",
    "            #计算loss\n",
    "            #将target也reshape成vector\n",
    "            #注意CrossEntropyLoss包含了LogSoftmax\n",
    "            #output: (seq_length * batch_size，vocab_size)\n",
    "            #target.view: (seq_length * batch_size)\n",
    "            #loss 为seq_length*batch_size个数据的平均loss\n",
    "            loss=loss_fn(output, target.view(-1))\n",
    "            #计算total loss\n",
    "            total_loss+=loss.item() * np.multiply(*data.size())\n",
    "            total_count+=np.multiply(*data.size())\n",
    "    \n",
    "    #评估完成后返回training模式\n",
    "    model.train()\n",
    "    \n",
    "    loss=total_loss/total_count\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练及保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0 :  10.816372871398926\n",
      "0 ,  evaluate loss:  10.80943772664499  min of val_losses:  Nan\n",
      "best model saved to ml.pth\n",
      "loss 100 :  7.199383735656738\n",
      "loss 200 :  7.657475471496582\n",
      "loss 300 :  7.2424821853637695\n",
      "loss 400 :  6.971253395080566\n",
      "loss 500 :  6.809700012207031\n",
      "loss 600 :  6.5486884117126465\n",
      "loss 700 :  6.947023868560791\n",
      "loss 800 :  6.5856828689575195\n",
      "loss 900 :  6.756621360778809\n",
      "loss 1000 :  6.849720001220703\n",
      "1000 ,  evaluate loss:  6.680168256303804  min of val_losses:  10.80943772664499\n",
      "best model saved to ml.pth\n",
      "loss 1100 :  6.598322868347168\n",
      "loss 1200 :  6.6317458152771\n",
      "loss 1300 :  6.6025519371032715\n",
      "loss 1400 :  6.171170234680176\n",
      "loss 1500 :  6.458670139312744\n",
      "loss 1600 :  6.4029741287231445\n",
      "loss 1700 :  6.377824306488037\n",
      "loss 1800 :  6.318821430206299\n",
      "loss 1900 :  6.365090370178223\n",
      "loss 2000 :  6.404265403747559\n",
      "2000 ,  evaluate loss:  6.42416395277828  min of val_losses:  6.680168256303804\n",
      "best model saved to ml.pth\n",
      "loss 2100 :  6.0813727378845215\n",
      "loss 2200 :  6.221803188323975\n",
      "loss 2300 :  6.359165191650391\n",
      "loss 2400 :  6.274023532867432\n",
      "loss 2500 :  6.168852806091309\n",
      "loss 2600 :  6.749006748199463\n",
      "loss 2700 :  6.409785270690918\n",
      "loss 2800 :  6.119002342224121\n",
      "loss 2900 :  6.34506368637085\n",
      "loss 3000 :  6.0721940994262695\n",
      "3000 ,  evaluate loss:  6.249363963838174  min of val_losses:  6.42416395277828\n",
      "best model saved to ml.pth\n",
      "loss 3100 :  6.4526801109313965\n",
      "loss 3200 :  6.539099216461182\n",
      "loss 3300 :  6.191751956939697\n",
      "loss 3400 :  6.121379852294922\n",
      "loss 3500 :  6.510777473449707\n",
      "loss 3600 :  6.195396423339844\n",
      "loss 3700 :  5.91970682144165\n",
      "loss 3800 :  6.22964334487915\n",
      "loss 3900 :  5.834619998931885\n",
      "loss 4000 :  6.1231207847595215\n",
      "4000 ,  evaluate loss:  6.124467911646695  min of val_losses:  6.249363963838174\n",
      "best model saved to ml.pth\n",
      "loss 4100 :  6.341670036315918\n",
      "loss 4200 :  6.093509197235107\n",
      "loss 4300 :  6.263672351837158\n",
      "loss 4400 :  6.2741780281066895\n",
      "loss 4500 :  6.41062068939209\n",
      "loss 4600 :  6.347156047821045\n",
      "loss 4700 :  6.356797218322754\n",
      "loss 4800 :  6.3056745529174805\n",
      "loss 4900 :  5.968372344970703\n",
      "loss 5000 :  5.968793869018555\n",
      "5000 ,  evaluate loss:  6.061663157068611  min of val_losses:  6.124467911646695\n",
      "best model saved to ml.pth\n",
      "loss 5100 :  6.2182297706604\n",
      "loss 5200 :  5.9479570388793945\n",
      "loss 5300 :  6.223063945770264\n",
      "loss 5400 :  6.397921085357666\n",
      "loss 5500 :  5.82716178894043\n",
      "loss 5600 :  6.167550086975098\n",
      "loss 5700 :  5.956594944000244\n",
      "loss 5800 :  6.2123122215271\n",
      "loss 5900 :  5.8719892501831055\n",
      "loss 6000 :  6.078254699707031\n",
      "6000 ,  evaluate loss:  5.9855577946606555  min of val_losses:  6.061663157068611\n",
      "best model saved to ml.pth\n",
      "loss 6100 :  5.873894214630127\n",
      "loss 6200 :  6.1607208251953125\n",
      "loss 6300 :  5.7220563888549805\n",
      "loss 6400 :  5.779325008392334\n",
      "loss 6500 :  5.742021083831787\n",
      "loss 6600 :  6.02327823638916\n",
      "loss 6700 :  6.291512966156006\n",
      "loss 6800 :  6.393104553222656\n",
      "loss 6900 :  6.520534515380859\n",
      "loss 7000 :  6.2280073165893555\n",
      "7000 ,  evaluate loss:  5.9179897134269055  min of val_losses:  5.9855577946606555\n",
      "best model saved to ml.pth\n",
      "loss 7100 :  5.669755935668945\n",
      "loss 7200 :  6.070861339569092\n",
      "loss 7300 :  5.881772041320801\n",
      "loss 7400 :  6.05290412902832\n",
      "loss 7500 :  6.014044284820557\n",
      "loss 7600 :  6.231567859649658\n",
      "loss 7700 :  5.898669242858887\n",
      "loss 7800 :  6.345977783203125\n",
      "loss 7900 :  6.13920783996582\n",
      "loss 8000 :  6.055931568145752\n",
      "8000 ,  evaluate loss:  5.873243102374093  min of val_losses:  5.9179897134269055\n",
      "best model saved to ml.pth\n",
      "loss 8100 :  5.94176721572876\n",
      "loss 8200 :  5.701865196228027\n",
      "loss 8300 :  5.845681667327881\n",
      "loss 8400 :  5.928764343261719\n",
      "loss 8500 :  5.6348700523376465\n",
      "loss 8600 :  5.959453105926514\n",
      "loss 8700 :  5.779302597045898\n",
      "loss 8800 :  6.108823299407959\n",
      "loss 8900 :  5.911745548248291\n",
      "loss 9000 :  5.813946723937988\n",
      "9000 ,  evaluate loss:  5.825792378306075  min of val_losses:  5.873243102374093\n",
      "best model saved to ml.pth\n",
      "loss 9100 :  6.298933982849121\n",
      "loss 9200 :  5.947091579437256\n",
      "loss 9300 :  6.184182167053223\n",
      "loss 9400 :  5.96766996383667\n",
      "loss 9500 :  5.854960918426514\n",
      "loss 9600 :  6.011432647705078\n",
      "loss 9700 :  5.719590663909912\n",
      "loss 9800 :  5.763672828674316\n",
      "loss 9900 :  5.988396644592285\n",
      "loss 10000 :  6.047870635986328\n",
      "10000 ,  evaluate loss:  5.79598048220553  min of val_losses:  5.825792378306075\n",
      "best model saved to ml.pth\n",
      "loss 10100 :  5.720191955566406\n",
      "loss 10200 :  5.872307300567627\n",
      "loss 10300 :  5.769929885864258\n",
      "loss 10400 :  6.088703155517578\n",
      "loss 10500 :  5.698253154754639\n",
      "loss 10600 :  5.9860968589782715\n",
      "loss 10700 :  5.800353050231934\n",
      "loss 10800 :  5.95564079284668\n",
      "loss 10900 :  5.926703929901123\n",
      "loss 11000 :  5.760790824890137\n",
      "11000 ,  evaluate loss:  5.7517233808371735  min of val_losses:  5.79598048220553\n",
      "best model saved to ml.pth\n",
      "loss 11100 :  5.973306179046631\n",
      "loss 11200 :  5.903067588806152\n",
      "loss 11300 :  5.8975019454956055\n",
      "loss 11400 :  5.717712879180908\n",
      "loss 11500 :  5.801076889038086\n",
      "loss 11600 :  5.83587646484375\n",
      "loss 11700 :  6.038607597351074\n",
      "loss 11800 :  6.031106948852539\n",
      "loss 11900 :  5.829713821411133\n",
      "loss 12000 :  5.992574691772461\n",
      "12000 ,  evaluate loss:  5.736199893312523  min of val_losses:  5.7517233808371735\n",
      "best model saved to ml.pth\n",
      "loss 12100 :  5.793646812438965\n",
      "loss 12200 :  5.854920387268066\n",
      "loss 12300 :  5.972712516784668\n",
      "loss 12400 :  6.3198981285095215\n",
      "loss 12500 :  6.154573917388916\n",
      "loss 12600 :  5.8568830490112305\n",
      "loss 12700 :  6.040863037109375\n",
      "loss 12800 :  6.066733360290527\n",
      "loss 12900 :  5.756881237030029\n",
      "loss 13000 :  5.985903263092041\n",
      "13000 ,  evaluate loss:  5.711466069234059  min of val_losses:  5.736199893312523\n",
      "best model saved to ml.pth\n",
      "loss 13100 :  5.877482891082764\n",
      "loss 13200 :  5.886585235595703\n",
      "loss 13300 :  5.959024906158447\n",
      "loss 13400 :  5.83113956451416\n",
      "loss 13500 :  5.982317924499512\n",
      "loss 13600 :  5.539343357086182\n",
      "loss 13700 :  5.761412620544434\n",
      "loss 13800 :  5.793429374694824\n",
      "loss 13900 :  5.502403259277344\n",
      "loss 14000 :  5.677840232849121\n",
      "14000 ,  evaluate loss:  5.676369850645399  min of val_losses:  5.711466069234059\n",
      "best model saved to ml.pth\n",
      "loss 14100 :  5.802210807800293\n",
      "loss 14200 :  5.882582664489746\n",
      "loss 14300 :  5.514610767364502\n",
      "loss 14400 :  5.490376949310303\n",
      "loss 14500 :  5.944432258605957\n",
      "loss 14600 :  5.873373031616211\n",
      "loss 14700 :  5.874460220336914\n",
      "loss 14800 :  5.716458320617676\n",
      "loss 14900 :  5.689104080200195\n",
      "loss 15000 :  5.858212947845459\n",
      "15000 ,  evaluate loss:  5.653596341811852  min of val_losses:  5.676369850645399\n",
      "best model saved to ml.pth\n",
      "loss 15100 :  6.009329319000244\n",
      "loss 15200 :  5.42269229888916\n",
      "loss 15300 :  5.5501837730407715\n",
      "loss 15400 :  5.832048416137695\n",
      "loss 15500 :  6.192506313323975\n",
      "loss 15600 :  6.013308525085449\n",
      "loss 15700 :  5.7795610427856445\n",
      "loss 15800 :  5.9241943359375\n",
      "loss 15900 :  5.932008743286133\n",
      "loss 16000 :  6.17210578918457\n",
      "16000 ,  evaluate loss:  5.63774986827019  min of val_losses:  5.653596341811852\n",
      "best model saved to ml.pth\n",
      "loss 16100 :  5.679257869720459\n",
      "loss 16200 :  5.4603424072265625\n",
      "loss 16300 :  5.826840400695801\n",
      "loss 16400 :  5.721621513366699\n",
      "loss 16500 :  5.856163501739502\n",
      "loss 16600 :  5.786683082580566\n",
      "loss 16700 :  6.12113618850708\n",
      "loss 16800 :  5.966250896453857\n",
      "loss 16900 :  5.891156196594238\n",
      "loss 17000 :  5.532505035400391\n",
      "17000 ,  evaluate loss:  5.61482050870207  min of val_losses:  5.63774986827019\n",
      "best model saved to ml.pth\n",
      "loss 17100 :  5.6680378913879395\n",
      "loss 17200 :  5.995782375335693\n",
      "loss 17300 :  5.5841898918151855\n",
      "loss 17400 :  5.525554656982422\n",
      "loss 17500 :  5.7819390296936035\n",
      "loss 17600 :  5.822617530822754\n",
      "loss 17700 :  5.288846969604492\n",
      "loss 17800 :  5.672390460968018\n",
      "loss 17900 :  5.705093860626221\n",
      "loss 18000 :  5.496434688568115\n",
      "18000 ,  evaluate loss:  5.596335240759974  min of val_losses:  5.61482050870207\n",
      "best model saved to ml.pth\n",
      "loss 18100 :  5.4966840744018555\n",
      "loss 18200 :  5.32185697555542\n",
      "loss 18300 :  6.174990177154541\n",
      "loss 18400 :  5.604557037353516\n",
      "loss 18500 :  5.572640419006348\n",
      "loss 18600 :  5.821869850158691\n",
      "loss 18700 :  5.9928693771362305\n",
      "loss 18800 :  5.461050987243652\n",
      "loss 18900 :  5.511395454406738\n",
      "loss 19000 :  5.8716044425964355\n",
      "19000 ,  evaluate loss:  5.5720693486321  min of val_losses:  5.596335240759974\n",
      "best model saved to ml.pth\n",
      "loss 19100 :  5.766670227050781\n",
      "loss 19200 :  5.739121437072754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 19300 :  5.998446464538574\n",
      "loss 19400 :  5.6315999031066895\n",
      "loss 19500 :  5.305489540100098\n",
      "loss 19600 :  5.405633449554443\n",
      "loss 19700 :  5.351130485534668\n",
      "loss 19800 :  5.340747833251953\n",
      "loss 19900 :  5.660774230957031\n",
      "loss 20000 :  5.5968122482299805\n",
      "20000 ,  evaluate loss:  5.548941659442814  min of val_losses:  5.5720693486321\n",
      "best model saved to ml.pth\n",
      "loss 20100 :  5.901849269866943\n",
      "loss 20200 :  4.952645301818848\n",
      "loss 20300 :  5.665374755859375\n",
      "loss 20400 :  5.764018535614014\n",
      "loss 20500 :  5.888459205627441\n",
      "loss 20600 :  5.764784336090088\n",
      "loss 20700 :  5.514475345611572\n",
      "loss 20800 :  5.5469465255737305\n",
      "loss 20900 :  5.783713340759277\n",
      "loss 21000 :  5.575328350067139\n",
      "21000 ,  evaluate loss:  5.52874816051766  min of val_losses:  5.548941659442814\n",
      "best model saved to ml.pth\n",
      "loss 21100 :  5.716557502746582\n",
      "loss 21200 :  5.741909503936768\n",
      "loss 21300 :  5.4770588874816895\n",
      "loss 21400 :  5.939389228820801\n",
      "loss 21500 :  5.746910095214844\n",
      "loss 21600 :  6.060485363006592\n",
      "loss 21700 :  6.043125152587891\n",
      "loss 21800 :  5.970810890197754\n",
      "loss 21900 :  5.518347263336182\n",
      "loss 22000 :  5.848729133605957\n",
      "22000 ,  evaluate loss:  5.520656923983929  min of val_losses:  5.52874816051766\n",
      "best model saved to ml.pth\n",
      "loss 22100 :  5.5237507820129395\n",
      "loss 22200 :  5.491692066192627\n",
      "loss 22300 :  5.622644424438477\n",
      "loss 22400 :  5.8677239418029785\n",
      "loss 22500 :  5.6317644119262695\n",
      "loss 22600 :  5.59884786605835\n",
      "loss 22700 :  6.118512153625488\n",
      "loss 22800 :  5.593975067138672\n",
      "loss 22900 :  5.424054145812988\n",
      "loss 23000 :  5.67254638671875\n",
      "23000 ,  evaluate loss:  5.503804390440321  min of val_losses:  5.520656923983929\n",
      "best model saved to ml.pth\n",
      "loss 23100 :  5.640633583068848\n",
      "loss 23200 :  5.485739707946777\n",
      "loss 23300 :  5.496247291564941\n",
      "loss 23400 :  5.205016136169434\n",
      "loss 23500 :  5.694451332092285\n",
      "loss 23600 :  5.644471168518066\n",
      "loss 23700 :  5.729464530944824\n",
      "loss 23800 :  5.661898612976074\n",
      "loss 23900 :  5.79591703414917\n",
      "loss 0 :  6.060070514678955\n",
      "0 ,  evaluate loss:  5.506475395486908  min of val_losses:  5.503804390440321\n",
      "learning rate decay\n",
      "loss 100 :  5.773253440856934\n",
      "loss 200 :  6.240263938903809\n",
      "loss 300 :  5.942479133605957\n",
      "loss 400 :  5.4659881591796875\n",
      "loss 500 :  5.664424419403076\n",
      "loss 600 :  5.447919845581055\n",
      "loss 700 :  5.668682098388672\n",
      "loss 800 :  5.572804927825928\n",
      "loss 900 :  5.771234512329102\n",
      "loss 1000 :  5.827136993408203\n",
      "1000 ,  evaluate loss:  5.479209149022904  min of val_losses:  5.503804390440321\n",
      "best model saved to ml.pth\n",
      "loss 1100 :  5.605186462402344\n",
      "loss 1200 :  5.5808234214782715\n",
      "loss 1300 :  5.731057167053223\n",
      "loss 1400 :  5.313355922698975\n",
      "loss 1500 :  5.564810276031494\n",
      "loss 1600 :  5.659463882446289\n",
      "loss 1700 :  5.7052412033081055\n",
      "loss 1800 :  5.560536861419678\n",
      "loss 1900 :  5.554902076721191\n",
      "loss 2000 :  5.66718864440918\n",
      "2000 ,  evaluate loss:  5.466890708907547  min of val_losses:  5.479209149022904\n",
      "best model saved to ml.pth\n",
      "loss 2100 :  5.326736927032471\n",
      "loss 2200 :  5.453665733337402\n",
      "loss 2300 :  5.534853935241699\n",
      "loss 2400 :  5.449251651763916\n",
      "loss 2500 :  5.503992557525635\n",
      "loss 2600 :  6.013541221618652\n",
      "loss 2700 :  5.775406837463379\n",
      "loss 2800 :  5.418308258056641\n",
      "loss 2900 :  5.558783531188965\n",
      "loss 3000 :  5.365513801574707\n",
      "3000 ,  evaluate loss:  5.454875184038684  min of val_losses:  5.466890708907547\n",
      "best model saved to ml.pth\n",
      "loss 3100 :  5.631399154663086\n",
      "loss 3200 :  5.721982479095459\n",
      "loss 3300 :  5.603115081787109\n",
      "loss 3400 :  5.494966506958008\n",
      "loss 3500 :  5.786108493804932\n",
      "loss 3600 :  5.502956390380859\n",
      "loss 3700 :  5.2342915534973145\n",
      "loss 3800 :  5.4920806884765625\n",
      "loss 3900 :  5.176208019256592\n",
      "loss 4000 :  5.575030326843262\n",
      "4000 ,  evaluate loss:  5.44030423087198  min of val_losses:  5.454875184038684\n",
      "best model saved to ml.pth\n",
      "loss 4100 :  5.695752143859863\n",
      "loss 4200 :  5.5468831062316895\n",
      "loss 4300 :  5.7720770835876465\n",
      "loss 4400 :  5.817986965179443\n",
      "loss 4500 :  5.745125770568848\n",
      "loss 4600 :  5.743373870849609\n",
      "loss 4700 :  5.877239227294922\n",
      "loss 4800 :  5.752743244171143\n",
      "loss 4900 :  5.489175796508789\n",
      "loss 5000 :  5.418080806732178\n",
      "5000 ,  evaluate loss:  5.4392491214294205  min of val_losses:  5.44030423087198\n",
      "best model saved to ml.pth\n",
      "loss 5100 :  5.698287010192871\n",
      "loss 5200 :  5.406785011291504\n",
      "loss 5300 :  5.584835529327393\n",
      "loss 5400 :  5.869056701660156\n",
      "loss 5500 :  5.269576549530029\n",
      "loss 5600 :  5.719046592712402\n",
      "loss 5700 :  5.5621209144592285\n",
      "loss 5800 :  5.687949180603027\n",
      "loss 5900 :  5.4040632247924805\n",
      "loss 6000 :  5.59493350982666\n",
      "6000 ,  evaluate loss:  5.431449277983333  min of val_losses:  5.4392491214294205\n",
      "best model saved to ml.pth\n",
      "loss 6100 :  5.346932888031006\n",
      "loss 6200 :  5.668642520904541\n",
      "loss 6300 :  5.2732672691345215\n",
      "loss 6400 :  5.423595428466797\n",
      "loss 6500 :  5.319293975830078\n",
      "loss 6600 :  5.578958034515381\n",
      "loss 6700 :  5.822455406188965\n",
      "loss 6800 :  5.906919956207275\n",
      "loss 6900 :  5.987626075744629\n",
      "loss 7000 :  5.7554402351379395\n",
      "7000 ,  evaluate loss:  5.421340638011032  min of val_losses:  5.431449277983333\n",
      "best model saved to ml.pth\n",
      "loss 7100 :  5.20946741104126\n",
      "loss 7200 :  5.6198320388793945\n",
      "loss 7300 :  5.4131574630737305\n",
      "loss 7400 :  5.576474189758301\n",
      "loss 7500 :  5.507269859313965\n",
      "loss 7600 :  5.73086404800415\n",
      "loss 7700 :  5.502486705780029\n",
      "loss 7800 :  5.904891014099121\n",
      "loss 7900 :  5.735691070556641\n",
      "loss 8000 :  5.765521049499512\n",
      "8000 ,  evaluate loss:  5.4141153648557365  min of val_losses:  5.421340638011032\n",
      "best model saved to ml.pth\n",
      "loss 8100 :  5.550121307373047\n",
      "loss 8200 :  5.296761512756348\n",
      "loss 8300 :  5.35008430480957\n",
      "loss 8400 :  5.569116115570068\n",
      "loss 8500 :  5.208558082580566\n",
      "loss 8600 :  5.553448677062988\n",
      "loss 8700 :  5.425692081451416\n",
      "loss 8800 :  5.706950664520264\n",
      "loss 8900 :  5.436033248901367\n",
      "loss 9000 :  5.466270446777344\n",
      "9000 ,  evaluate loss:  5.409230485283705  min of val_losses:  5.4141153648557365\n",
      "best model saved to ml.pth\n",
      "loss 9100 :  5.916886329650879\n",
      "loss 9200 :  5.555251121520996\n",
      "loss 9300 :  5.738894939422607\n",
      "loss 9400 :  5.562784671783447\n",
      "loss 9500 :  5.430973529815674\n",
      "loss 9600 :  5.663172721862793\n",
      "loss 9700 :  5.374512672424316\n",
      "loss 9800 :  5.354504585266113\n",
      "loss 9900 :  5.392088890075684\n",
      "loss 10000 :  5.667550086975098\n",
      "10000 ,  evaluate loss:  5.413379293031875  min of val_losses:  5.409230485283705\n",
      "learning rate decay\n",
      "loss 10100 :  5.396346092224121\n",
      "loss 10200 :  5.494434833526611\n",
      "loss 10300 :  5.44242525100708\n",
      "loss 10400 :  5.766385078430176\n",
      "loss 10500 :  5.347601890563965\n",
      "loss 10600 :  5.584420204162598\n",
      "loss 10700 :  5.465917110443115\n",
      "loss 10800 :  5.547869682312012\n",
      "loss 10900 :  5.594616889953613\n",
      "loss 11000 :  5.395543575286865\n",
      "11000 ,  evaluate loss:  5.395084632712735  min of val_losses:  5.409230485283705\n",
      "best model saved to ml.pth\n",
      "loss 11100 :  5.57438325881958\n",
      "loss 11200 :  5.611685276031494\n",
      "loss 11300 :  5.480260372161865\n",
      "loss 11400 :  5.314478397369385\n",
      "loss 11500 :  5.503951072692871\n",
      "loss 11600 :  5.491756916046143\n",
      "loss 11700 :  5.688017845153809\n",
      "loss 11800 :  5.712826728820801\n",
      "loss 11900 :  5.52446174621582\n",
      "loss 12000 :  5.6937456130981445\n",
      "12000 ,  evaluate loss:  5.393467004737954  min of val_losses:  5.395084632712735\n",
      "best model saved to ml.pth\n",
      "loss 12100 :  5.501091003417969\n",
      "loss 12200 :  5.566651344299316\n",
      "loss 12300 :  5.691442489624023\n",
      "loss 12400 :  5.96985387802124\n",
      "loss 12500 :  5.848250389099121\n",
      "loss 12600 :  5.537758827209473\n",
      "loss 12700 :  5.650245666503906\n",
      "loss 12800 :  5.818148136138916\n",
      "loss 12900 :  5.50111198425293\n",
      "loss 13000 :  5.663119316101074\n",
      "13000 ,  evaluate loss:  5.393200527782677  min of val_losses:  5.393467004737954\n",
      "best model saved to ml.pth\n",
      "loss 13100 :  5.595887660980225\n",
      "loss 13200 :  5.602812767028809\n",
      "loss 13300 :  5.6254730224609375\n",
      "loss 13400 :  5.478049278259277\n",
      "loss 13500 :  5.587098121643066\n",
      "loss 13600 :  5.269444465637207\n",
      "loss 13700 :  5.581963539123535\n",
      "loss 13800 :  5.493605136871338\n",
      "loss 13900 :  5.29384708404541\n",
      "loss 14000 :  5.419078350067139\n",
      "14000 ,  evaluate loss:  5.385304015428445  min of val_losses:  5.393200527782677\n",
      "best model saved to ml.pth\n",
      "loss 14100 :  5.5588555335998535\n",
      "loss 14200 :  5.66517448425293\n",
      "loss 14300 :  5.281201362609863\n",
      "loss 14400 :  5.222289085388184\n",
      "loss 14500 :  5.6022257804870605\n",
      "loss 14600 :  5.525507926940918\n",
      "loss 14700 :  5.578863620758057\n",
      "loss 14800 :  5.474776268005371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 14900 :  5.453059196472168\n",
      "loss 15000 :  5.6264967918396\n",
      "15000 ,  evaluate loss:  5.385779374337618  min of val_losses:  5.385304015428445\n",
      "learning rate decay\n",
      "loss 15100 :  5.789181709289551\n",
      "loss 15200 :  5.269637107849121\n",
      "loss 15300 :  5.307678699493408\n",
      "loss 15400 :  5.61185884475708\n",
      "loss 15500 :  5.940357208251953\n",
      "loss 15600 :  5.8243632316589355\n",
      "loss 15700 :  5.576089859008789\n",
      "loss 15800 :  5.648012161254883\n",
      "loss 15900 :  5.709059715270996\n",
      "loss 16000 :  5.940976619720459\n",
      "16000 ,  evaluate loss:  5.37948165300604  min of val_losses:  5.385304015428445\n",
      "best model saved to ml.pth\n",
      "loss 16100 :  5.498838901519775\n",
      "loss 16200 :  5.255729675292969\n",
      "loss 16300 :  5.576426029205322\n",
      "loss 16400 :  5.472335338592529\n",
      "loss 16500 :  5.629510879516602\n",
      "loss 16600 :  5.495746612548828\n",
      "loss 16700 :  5.847012519836426\n",
      "loss 16800 :  5.7742204666137695\n",
      "loss 16900 :  5.686188220977783\n",
      "loss 17000 :  5.3066840171813965\n",
      "17000 ,  evaluate loss:  5.375715143930925  min of val_losses:  5.37948165300604\n",
      "best model saved to ml.pth\n",
      "loss 17100 :  5.458439826965332\n",
      "loss 17200 :  5.751410007476807\n",
      "loss 17300 :  5.356442928314209\n",
      "loss 17400 :  5.301050662994385\n",
      "loss 17500 :  5.57926607131958\n",
      "loss 17600 :  5.659409523010254\n",
      "loss 17700 :  5.126911640167236\n",
      "loss 17800 :  5.454145908355713\n",
      "loss 17900 :  5.4570770263671875\n",
      "loss 18000 :  5.335550308227539\n",
      "18000 ,  evaluate loss:  5.37245273195266  min of val_losses:  5.375715143930925\n",
      "best model saved to ml.pth\n",
      "loss 18100 :  5.295271873474121\n",
      "loss 18200 :  5.1008620262146\n",
      "loss 18300 :  5.9277424812316895\n",
      "loss 18400 :  5.3817644119262695\n",
      "loss 18500 :  5.3443403244018555\n",
      "loss 18600 :  5.578577518463135\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-0475f0928b4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m#在语言模型中， 训练集中的前一个句子与后一个句子是相连的\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m#所以下一个batch/iteration/下一个backpropagationThroughTime的过程仍然可以用上一次的hidden state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m#output形状：(seq_length， batch_size， vocab_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venv_pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-e645a22dce45>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_text, hidden)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m#embedding传入LSTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m#hidden: hidden state & cell state 两者形状相同\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;31m# output: seq_length * batch_size * hidden_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# hidden: (1*batch_size*hiddensize, 1*batch_size*hidden_size) 1: LSTM层数为1, hidden state 参数同LSTMdancing的输出的形状相同\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venv_pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venv_pytorch\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    678\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[1;32m--> 680\u001b[1;33m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[0;32m    681\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHES):\n",
    "    #Sets the module in training mode.\n",
    "    model.train()\n",
    "    #将train_iter转化为迭代器\n",
    "    it=iter(train_iter)\n",
    "    #初始化hidden state\n",
    "    hidden= model.init_hidden(BATCH_SIZE)\n",
    "    #enumerate: 为迭代器每次迭代添加序号\n",
    "    for i, batch in enumerate(it):\n",
    "        data, target = batch.text, batch.target #已经在cuda上了，不需要进行设备转换: print(batch.text)\n",
    "        \n",
    "        #在每个batch调用hidden之前，将hidden与其之前的历史分离\n",
    "        #保证虽然利用了之前的hidden，但是bptt只在此次batch中进行\n",
    "        hidden=repackage_hidden(hidden)\n",
    "        \n",
    "        #在语言模型中， 训练集中的前一个句子与后一个句子是相连的\n",
    "        #所以下一个batch/iteration/下一个backpropagationThroughTime的过程仍然可以用上一次的hidden state\n",
    "        output, hidden =model(data, hidden)\n",
    "        \n",
    "        #output形状：(seq_length， batch_size， vocab_size)\n",
    "        #为了使用crossentropy计算loss，需要对output reshape为(seq_length * batch_size，vocab_size)\n",
    "        output=output.reshape(-1, VOCAB_SIZE)\n",
    "        \n",
    "        #计算loss\n",
    "        #将target也reshape成vector\n",
    "        #注意CrossEntropyLoss包含了LogSoftmax\n",
    "        #output: (seq_length * batch_size，vocab_size)\n",
    "        #garget.view: (seq_length * batch_size)\n",
    "        loss=loss_fn(output, target.view(-1))\n",
    "        \n",
    "        #backward\n",
    "        loss.backward()\n",
    "        \n",
    "        #将parameters clip，防止vanishing gradients and exploding gradients.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "        \n",
    "        #更新网络参数\n",
    "        optimizer.step()\n",
    "        \n",
    "        #清零gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #每100次输出loss\n",
    "        if i%100==0:\n",
    "            print('loss', i, ': ', loss.item())\n",
    "            \n",
    "            \n",
    "        ####################\n",
    "        # 保存模型\n",
    "        ####################\n",
    "        #每1000个iteration保存一次\n",
    "        if i%1000 ==0:\n",
    "            #在validation数据集上评估模型\n",
    "            val_loss=evaluate(model, val_iter)\n",
    "            print(i,', ','evaluate loss: ', val_loss, ' min of val_losses: ', min(val_losses) if len(val_losses)!=0 else 'Nan')\n",
    "            #如果是第一次评估，或者评估结果比之前都要好，则保存模型\n",
    "            if len(val_losses)==0 or val_loss < min(val_losses):\n",
    "                #model.state_dict(): OrdereDict, 保存有模型的所有参数\n",
    "                #'language_model.pth' ： 模型名称\n",
    "                torch.save(model.state_dict(), \"language_model.pth\")\n",
    "                print('best model saved to ml.pth')\n",
    "            #否则评估结果比之前最好的结果差，说明learning rate可能过大\n",
    "            #可以通过调节learning rate让loss继续下降\n",
    "            else:\n",
    "                scheduler.step()\n",
    "                print('learning rate decay')\n",
    "            val_losses.append(val_loss)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('embed.weight', tensor([[-0.9702, -0.6682, -0.5829,  ...,  0.9879,  0.3909, -0.5976],\n",
      "        [ 1.4040, -0.7950, -0.9087,  ..., -1.8664, -2.8979, -0.5862],\n",
      "        [-0.5034,  1.0965, -0.7679,  ...,  0.1064,  1.1298,  0.9128],\n",
      "        ...,\n",
      "        [ 0.5915,  0.3283,  1.0743,  ..., -1.4168,  0.7816,  0.3870],\n",
      "        [ 2.4916,  1.0625,  1.0658,  ...,  1.6001,  0.7269, -0.3997],\n",
      "        [ 0.2299, -0.2738, -0.2193,  ...,  0.0397,  1.6360, -0.7308]],\n",
      "       device='cuda:0')), ('lstm.weight_ih_l0', tensor([[ 0.5405,  0.2128, -0.4492,  ..., -0.1526, -0.0898, -0.1378],\n",
      "        [ 0.0942,  0.2235,  0.0158,  ...,  0.3332, -0.0449, -0.0307],\n",
      "        [ 0.1211, -0.0356,  0.1027,  ...,  0.1102,  0.0684, -0.0661],\n",
      "        ...,\n",
      "        [ 0.1776, -0.1508,  0.2254,  ...,  0.2558,  0.2720,  0.6122],\n",
      "        [-0.1351, -0.0350,  0.0908,  ...,  0.0683,  0.0310, -0.2215],\n",
      "        [-0.1463,  0.1297, -0.0244,  ..., -0.0258, -0.0372,  0.0979]],\n",
      "       device='cuda:0')), ('lstm.weight_hh_l0', tensor([[ 0.2083, -0.1944,  0.2916,  ..., -0.3407,  0.2152, -0.0201],\n",
      "        [-0.4545,  0.3413,  0.0260,  ..., -0.1181,  0.0553,  0.1878],\n",
      "        [ 0.1541, -0.1481,  0.0044,  ...,  0.3587, -0.0440,  0.0252],\n",
      "        ...,\n",
      "        [-0.0634,  0.2306, -0.0185,  ...,  0.3244, -0.0844, -0.1621],\n",
      "        [-0.1577,  0.2786,  0.3015,  ...,  0.6519,  0.8931, -0.4823],\n",
      "        [ 0.1063, -0.0699,  0.3220,  ...,  0.5262,  0.0733, -0.0431]],\n",
      "       device='cuda:0')), ('lstm.bias_ih_l0', tensor([-2.2432e-01,  2.5286e-01,  2.7757e-01, -3.6678e-01,  2.0370e-01,\n",
      "        -1.3042e-01, -7.3612e-01, -3.8864e-02, -2.0762e-01, -9.1523e-02,\n",
      "        -4.3998e-01, -7.8439e-01,  1.2212e-01, -1.3712e-01,  2.1596e-01,\n",
      "        -1.2092e-01, -9.8554e-02, -3.0881e-01,  6.4588e-02, -1.8167e-01,\n",
      "        -1.5347e-01,  1.4404e-01,  4.4565e-01, -9.2474e-01, -2.6715e-01,\n",
      "        -2.0438e-01,  3.0355e-01, -7.7679e-02, -9.3729e-01,  6.6361e-02,\n",
      "        -3.7126e-01, -4.8377e-02,  2.0571e-02, -3.1172e-02,  3.4515e-01,\n",
      "        -9.5263e-01,  6.1665e-01, -2.3825e-01,  4.7109e-01,  2.5711e-02,\n",
      "         2.3663e-01,  2.5330e-03,  2.9755e-01,  3.2673e-01,  3.8827e-02,\n",
      "        -4.7838e-02, -6.6835e-01, -9.5540e-01,  2.3563e-01, -1.8382e-01,\n",
      "        -4.5446e-01,  6.1881e-01, -5.1322e-01, -2.3538e-01, -2.6905e-01,\n",
      "        -2.2741e-01, -7.9567e-03,  2.2032e-03, -4.2921e-03, -5.3679e-01,\n",
      "         3.4259e-02,  3.2303e-02, -2.7508e-01, -1.4321e-01,  5.2759e-01,\n",
      "        -4.7760e-01, -2.8259e-01,  3.2695e-02,  1.8842e-02, -2.4034e-02,\n",
      "        -5.2975e-01, -2.5328e-01, -5.5627e-01,  2.7831e-01,  2.0100e-01,\n",
      "        -1.6510e-01, -2.7606e-02,  6.8734e-02,  4.1239e-01, -2.0592e-02,\n",
      "         3.7956e-02, -1.7511e-01, -2.2377e-01,  1.3344e-01, -6.8648e-01,\n",
      "         6.2461e-01,  3.1957e-01,  4.2071e-01, -3.9848e-01,  2.2957e-01,\n",
      "        -4.5353e-02,  4.5468e-01,  9.8140e-03, -4.2628e-01,  5.8165e-01,\n",
      "        -2.4367e-02, -4.6016e-02, -6.7118e-01, -7.1386e-01, -1.7267e-01,\n",
      "        -4.7606e-01,  2.4595e-02, -1.0525e-01,  1.2021e-01, -1.7819e-01,\n",
      "        -7.0272e-01,  2.4493e-01,  1.4521e-01, -3.3309e-01, -2.6615e-02,\n",
      "         7.9577e-01,  8.2253e-01, -6.9809e-01,  2.3446e-01, -2.9219e-01,\n",
      "        -8.9130e-01, -4.8490e-01, -1.3542e-01, -5.5331e-01, -3.0892e-01,\n",
      "         1.4059e-01, -3.7966e-01, -4.2859e-01,  7.3495e-01, -2.9637e-01,\n",
      "        -6.4289e-01, -1.0954e-01, -7.1114e-01,  9.1698e-01, -3.1480e-03,\n",
      "        -1.9082e-01, -5.1555e-01, -2.1514e-02, -6.0521e-01, -5.5473e-01,\n",
      "         1.0889e+00, -3.3290e-01, -5.1793e-01, -4.0843e-01, -3.9271e-01,\n",
      "        -9.1604e-01, -4.9411e-02, -3.9503e-01, -4.9666e-01, -3.0106e-01,\n",
      "        -2.0589e-01,  9.3920e-01,  1.0457e+00,  4.5783e-01, -2.0492e-01,\n",
      "         6.3483e-01, -2.2796e-01, -6.9051e-01,  1.3658e-01, -5.4180e-01,\n",
      "        -4.6091e-01, -3.8331e-01, -4.9403e-01, -5.1546e-02, -3.7237e-02,\n",
      "        -1.5764e-01,  1.1232e-01, -3.7049e-01, -6.0933e-01, -5.9564e-01,\n",
      "        -3.4297e-01, -2.7421e-01,  2.7555e-01, -7.9542e-01, -4.1293e-01,\n",
      "         7.5994e-01, -2.2458e-01,  4.6992e-01, -1.1680e-01, -4.0319e-01,\n",
      "        -3.4635e-01, -2.5607e-01, -3.6883e-01, -1.9009e-01, -2.3329e-01,\n",
      "         3.8605e-01, -1.6122e-01, -2.4837e-01, -5.5725e-01,  9.1317e-01,\n",
      "        -6.5407e-02, -8.8025e-01, -1.2707e-01,  3.2957e-01, -1.4645e-01,\n",
      "        -2.4409e-01, -6.8857e-01, -6.9630e-01,  5.0700e-01, -1.9701e-01,\n",
      "        -1.8139e-01,  1.7670e-01,  3.9015e-02,  7.9945e-01,  5.3200e-01,\n",
      "        -7.0260e-01,  4.9000e-01,  5.2297e-01,  4.3131e-02, -2.2625e-02,\n",
      "         2.5184e-01,  1.7284e-01,  6.3072e-01,  4.8002e-01,  6.6012e-01,\n",
      "        -1.2173e-01,  5.0950e-02,  5.0542e-02,  4.8665e-01, -2.1804e-01,\n",
      "         4.0471e-01, -9.1794e-02, -4.8847e-01,  5.2208e-01, -1.3991e-01,\n",
      "         5.2726e-01,  1.0962e-01, -4.0963e-01, -3.4229e-01,  5.2947e-01,\n",
      "        -1.3259e-01,  4.4873e-02,  4.5444e-01,  1.6290e-01, -3.4198e-01,\n",
      "         4.5374e-01,  4.4994e-01,  2.1594e-02,  1.4005e-01, -3.2349e-01,\n",
      "         2.8805e-01, -5.1413e-01, -3.8337e-01, -5.3499e-01,  3.3008e-01,\n",
      "        -5.7904e-02, -2.4608e-01,  3.7679e-01, -5.9478e-02, -1.5456e-01,\n",
      "         4.9323e-01, -3.9470e-02, -2.1552e-02, -6.5687e-02, -3.0267e-01,\n",
      "         1.2408e-01,  1.0726e-01, -1.2465e-01, -6.7133e-01,  1.2851e-01,\n",
      "         3.5324e-01, -5.3670e-01,  2.0916e-01,  7.4446e-02, -2.5255e-01,\n",
      "         5.3637e-01,  3.4382e-01,  4.0841e-01, -6.2658e-01,  1.2640e-02,\n",
      "        -3.0710e-01,  1.7660e-01, -2.4817e-01,  4.4979e-02, -2.8516e-01,\n",
      "        -6.0898e-02,  4.8154e-01,  8.5392e-02, -5.3578e-01,  1.7463e-01,\n",
      "         4.7809e-01, -6.0008e-01, -1.3655e-01, -5.3087e-01, -2.2556e-01,\n",
      "         2.5006e-01,  3.1943e-01,  2.8288e-01,  5.7419e-01,  2.0102e-01,\n",
      "        -4.8122e-01, -3.8897e-01,  4.7935e-01, -2.1509e-01, -3.5810e-01,\n",
      "        -3.1514e-01, -1.5733e-03,  5.5152e-01,  3.1654e-01, -5.1776e-01,\n",
      "        -4.7607e-01,  2.4078e-01,  4.2391e-01, -9.9334e-02, -1.5424e-01,\n",
      "         4.8442e-01,  3.2710e-01,  7.1746e-01,  9.2408e-01,  7.2372e-01,\n",
      "        -7.6350e-01,  6.2806e-01,  5.5146e-01,  3.9997e-01, -1.4613e+00,\n",
      "         1.8744e+00,  1.0761e+00, -4.7846e-02, -6.3841e-01, -4.9794e-02,\n",
      "        -4.0007e-01,  1.4014e-01, -1.1237e+00, -5.9155e-01, -1.2702e+00,\n",
      "        -4.9065e-01,  6.2868e-01,  2.5595e-01,  8.3973e-01, -7.7954e-01,\n",
      "        -3.2749e-02,  8.2075e-01,  9.9947e-01,  1.8289e+00,  8.5722e-01,\n",
      "        -5.7477e-02, -1.3994e+00,  6.4465e-01,  5.8782e-03,  4.7672e-01,\n",
      "         1.6451e+00,  6.1435e-01, -9.8250e-01, -7.2547e-02,  3.5389e-01,\n",
      "         1.6982e-01,  2.2429e-01,  2.4444e-02,  2.0394e-01, -1.6863e+00,\n",
      "        -7.3873e-01,  1.9341e+00,  2.0277e+00,  9.4707e-01, -1.0963e+00,\n",
      "         1.4923e+00,  1.1992e+00, -1.2219e+00,  2.2587e-01, -9.3074e-01,\n",
      "        -4.3738e-01,  4.2321e-02, -1.5249e+00,  7.7382e-01,  4.2415e-01,\n",
      "        -1.8122e+00,  2.6541e-01, -1.5103e+00, -2.7344e-01,  5.7539e-01,\n",
      "         6.2368e-01, -1.3279e-01,  8.6536e-01, -5.3035e-01, -5.5568e-01,\n",
      "         1.4091e+00, -1.2867e+00, -1.6560e-01, -4.9999e-01,  3.8646e-01,\n",
      "        -6.3252e-01, -4.5284e-02,  6.9855e-01,  3.5676e-01, -1.4325e+00,\n",
      "         1.4455e+00,  1.0312e+00, -1.4382e+00,  1.9332e-02,  1.7067e+00,\n",
      "         7.6236e-01,  5.2559e-01, -1.2031e+00,  3.5816e-01,  7.5768e-01,\n",
      "        -1.0099e+00,  4.0354e-01, -4.7427e-01,  1.4211e+00,  7.2486e-01,\n",
      "         3.3579e-01,  7.3718e-01, -2.0046e-01,  1.7232e+00,  1.2322e+00],\n",
      "       device='cuda:0')), ('lstm.bias_hh_l0', tensor([-0.2829,  0.2028,  0.2643, -0.3673,  0.1131, -0.1982, -0.5937,  0.0394,\n",
      "        -0.0483, -0.0994, -0.4924, -0.9205,  0.1679, -0.0204,  0.1178, -0.2008,\n",
      "        -0.1834, -0.2262,  0.1538, -0.1653, -0.2412,  0.1873,  0.4166, -0.8620,\n",
      "        -0.1678, -0.1260,  0.3304, -0.0916, -0.8407, -0.0820, -0.2409,  0.0041,\n",
      "        -0.0339,  0.0186,  0.3529, -1.0475,  0.5047, -0.1937,  0.4580,  0.0317,\n",
      "         0.2532, -0.0280,  0.1703,  0.3765,  0.1370,  0.0561, -0.7231, -0.9437,\n",
      "         0.1789, -0.2428, -0.5486,  0.6305, -0.4497, -0.3081, -0.2279, -0.1663,\n",
      "        -0.0140, -0.0980, -0.1203, -0.5123, -0.0318, -0.0340, -0.2351, -0.0613,\n",
      "         0.5375, -0.5642, -0.1693,  0.1289, -0.0476, -0.0231, -0.4632, -0.1822,\n",
      "        -0.7136,  0.3055,  0.1837, -0.1661, -0.0452,  0.2145,  0.4053, -0.0520,\n",
      "        -0.0692, -0.2341, -0.0455,  0.1288, -0.8130,  0.4650,  0.3875,  0.3172,\n",
      "        -0.3908,  0.1725, -0.1185,  0.2794,  0.0954, -0.4375,  0.6726,  0.0133,\n",
      "        -0.0802, -0.7636, -0.5462, -0.1991, -0.4055, -0.1442, -0.2298,  0.2560,\n",
      "        -0.0484, -0.6607,  0.1448,  0.2495, -0.4849,  0.0464,  0.8555,  0.8945,\n",
      "        -0.7088,  0.3967, -0.3443, -0.9808, -0.4529, -0.1548, -0.6817, -0.3537,\n",
      "         0.0600, -0.2668, -0.5136,  0.6217, -0.1840, -0.5571, -0.1409, -0.7691,\n",
      "         1.0792,  0.1269, -0.2733, -0.4824,  0.0145, -0.5828, -0.6391,  1.1712,\n",
      "        -0.3789, -0.4641, -0.4931, -0.3869, -0.9043, -0.1147, -0.4108, -0.5289,\n",
      "        -0.2102, -0.1478,  0.9656,  1.1082,  0.5901, -0.3215,  0.6371, -0.0947,\n",
      "        -0.8050,  0.1378, -0.5783, -0.3436, -0.2579, -0.5553, -0.1067,  0.0091,\n",
      "        -0.1082,  0.0239, -0.3442, -0.6007, -0.4500, -0.1515, -0.2127,  0.2501,\n",
      "        -0.8369, -0.4839,  0.7045, -0.2816,  0.5361, -0.1862, -0.3035, -0.3058,\n",
      "        -0.2244, -0.4537, -0.1388, -0.3362,  0.3453, -0.0246, -0.3981, -0.5785,\n",
      "         1.0178, -0.1164, -0.8634, -0.0803,  0.4758, -0.2340, -0.2868, -0.6049,\n",
      "        -0.7750,  0.5175, -0.2782, -0.2788,  0.3144,  0.1470,  0.8147,  0.4389,\n",
      "        -0.6577,  0.4215,  0.4627, -0.0252, -0.0412,  0.1569,  0.0508,  0.6538,\n",
      "         0.5232,  0.5763, -0.0526,  0.0715, -0.0577,  0.5415, -0.2977,  0.3143,\n",
      "        -0.1386, -0.3812,  0.5978, -0.2547,  0.5382,  0.1867, -0.4495, -0.2234,\n",
      "         0.4405, -0.2389, -0.0178,  0.3396,  0.1532, -0.1982,  0.4374,  0.4381,\n",
      "        -0.1454,  0.1238, -0.3014,  0.1382, -0.4827, -0.4102, -0.5942,  0.5045,\n",
      "        -0.1517, -0.1075,  0.3296, -0.1380, -0.0821,  0.5829,  0.0382, -0.0251,\n",
      "         0.1262, -0.3081,  0.0456,  0.1901, -0.2706, -0.5500,  0.0827,  0.4254,\n",
      "        -0.6111,  0.2272,  0.0596, -0.3895,  0.6033,  0.3955,  0.3575, -0.5818,\n",
      "         0.0158, -0.3273,  0.3011, -0.1171, -0.0479, -0.4503, -0.1474,  0.4636,\n",
      "         0.1168, -0.5250,  0.2079,  0.4801, -0.4513, -0.2031, -0.4916, -0.2085,\n",
      "         0.1484,  0.3134,  0.2613,  0.6010,  0.0456, -0.5553, -0.3682,  0.4499,\n",
      "        -0.1465, -0.3191, -0.4128,  0.0970,  0.5738,  0.3541, -0.5582, -0.6438,\n",
      "         0.2557,  0.2761, -0.0848, -0.0960,  0.6522,  0.2743,  0.7545,  1.0467,\n",
      "         0.7717, -0.8210,  0.7551,  0.6300,  0.2612, -1.4446,  1.7364,  1.1311,\n",
      "        -0.1400, -0.7046,  0.0449, -0.3344,  0.1507, -1.1286, -0.5974, -1.2036,\n",
      "        -0.4680,  0.6006,  0.2643,  0.8328, -0.8532, -0.0543,  0.9449,  1.0961,\n",
      "         1.6905,  0.8579, -0.0311, -1.3087,  0.5875,  0.1587,  0.3698,  1.5564,\n",
      "         0.6675, -0.9816, -0.0294,  0.5173,  0.0408,  0.3960, -0.0044,  0.2358,\n",
      "        -1.6123, -0.8626,  1.9146,  1.9595,  0.9281, -1.2043,  1.5471,  1.2910,\n",
      "        -1.2275,  0.1879, -0.7346, -0.4012,  0.0890, -1.6315,  0.9383,  0.4073,\n",
      "        -1.7264,  0.3432, -1.4412, -0.2708,  0.6026,  0.7470, -0.0796,  0.8397,\n",
      "        -0.5528, -0.6132,  1.4579, -1.3511, -0.0059, -0.5250,  0.4239, -0.6440,\n",
      "        -0.1968,  0.7340,  0.2503, -1.4217,  1.4696,  1.0728, -1.4436,  0.0099,\n",
      "         1.6870,  0.8120,  0.4620, -1.2218,  0.3449,  0.7569, -0.8723,  0.5112,\n",
      "        -0.4884,  1.4074,  0.6280,  0.3499,  0.6838, -0.1837,  1.8026,  1.1417],\n",
      "       device='cuda:0')), ('linear.weight', tensor([[-0.0673,  0.0585,  0.3294,  ..., -0.0224, -0.0385, -0.0121],\n",
      "        [ 0.2508, -0.1607, -1.0444,  ..., -1.1814,  0.2383,  0.2945],\n",
      "        [ 0.7005,  0.4724,  0.7970,  ...,  0.0165, -0.0760, -0.1357],\n",
      "        ...,\n",
      "        [-0.2235, -0.3216, -0.6124,  ..., -1.8667,  0.4452,  0.0899],\n",
      "        [ 0.0353, -0.2041, -0.3577,  ..., -1.3757,  0.3075,  0.2015],\n",
      "        [ 0.2621, -0.2651, -0.5862,  ..., -1.4826,  0.1758,  0.2480]],\n",
      "       device='cuda:0')), ('linear.bias', tensor([ 3.7014, -4.2577,  2.6139,  ..., -1.5984, -0.6576, -0.6872],\n",
      "       device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model=RNNModel(vocab_size=len(TEXT.vocab), \n",
    "                    embed_size=EMBEDDING_SIZE, \n",
    "                    hidden_size=HIDDEN_SIZE)\n",
    "if USE_CUDA:\n",
    "    best_model=best_model.to(device)\n",
    "\n",
    "best_model.load_state_dict(torch.load('language_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('embed.weight', tensor([[-0.9659, -0.6699, -0.5871,  ...,  0.9922,  0.3876, -0.5927],\n",
      "        [ 1.4040, -0.7950, -0.9087,  ..., -1.8664, -2.8979, -0.5862],\n",
      "        [-0.5051,  1.0984, -0.7697,  ...,  0.0966,  1.1286,  0.9072],\n",
      "        ...,\n",
      "        [ 0.5915,  0.3283,  1.0743,  ..., -1.4168,  0.7816,  0.3870],\n",
      "        [ 2.4916,  1.0625,  1.0658,  ...,  1.6001,  0.7269, -0.3997],\n",
      "        [ 0.2299, -0.2738, -0.2193,  ...,  0.0397,  1.6360, -0.7308]],\n",
      "       device='cuda:0')), ('lstm.weight_ih_l0', tensor([[ 0.5379,  0.2186, -0.4546,  ..., -0.1507, -0.0914, -0.1355],\n",
      "        [ 0.0958,  0.2222,  0.0127,  ...,  0.3348, -0.0509, -0.0362],\n",
      "        [ 0.1263, -0.0298,  0.0984,  ...,  0.1136,  0.0658, -0.0637],\n",
      "        ...,\n",
      "        [ 0.1749, -0.1459,  0.2257,  ...,  0.2547,  0.2676,  0.6108],\n",
      "        [-0.1373, -0.0316,  0.0896,  ...,  0.0654,  0.0321, -0.2204],\n",
      "        [-0.1424,  0.1321, -0.0286,  ..., -0.0247, -0.0383,  0.0964]],\n",
      "       device='cuda:0')), ('lstm.weight_hh_l0', tensor([[ 0.2105, -0.1945,  0.2922,  ..., -0.3471,  0.2146, -0.0137],\n",
      "        [-0.4528,  0.3422,  0.0275,  ..., -0.1213,  0.0462,  0.1898],\n",
      "        [ 0.1525, -0.1468,  0.0081,  ...,  0.3635, -0.0454,  0.0243],\n",
      "        ...,\n",
      "        [-0.0598,  0.2327, -0.0088,  ...,  0.3306, -0.0934, -0.1566],\n",
      "        [-0.1546,  0.2851,  0.3029,  ...,  0.6542,  0.8904, -0.4823],\n",
      "        [ 0.1068, -0.0716,  0.3187,  ...,  0.5260,  0.0672, -0.0403]],\n",
      "       device='cuda:0')), ('lstm.bias_ih_l0', tensor([-2.2058e-01,  2.5058e-01,  2.8251e-01, -3.6696e-01,  2.0342e-01,\n",
      "        -1.2570e-01, -7.3560e-01, -3.9825e-02, -2.1613e-01, -8.6432e-02,\n",
      "        -4.4274e-01, -7.8279e-01,  1.2294e-01, -1.3203e-01,  2.1941e-01,\n",
      "        -1.1605e-01, -9.6129e-02, -3.0994e-01,  6.5196e-02, -1.7989e-01,\n",
      "        -1.5653e-01,  1.4558e-01,  4.5151e-01, -9.1550e-01, -2.6446e-01,\n",
      "        -2.0945e-01,  2.9898e-01, -7.8670e-02, -9.3380e-01,  5.6529e-02,\n",
      "        -3.6924e-01, -4.6696e-02,  8.8113e-03, -3.1214e-02,  3.4435e-01,\n",
      "        -9.4735e-01,  6.1182e-01, -2.3900e-01,  4.6875e-01,  2.4732e-02,\n",
      "         2.3315e-01,  7.2624e-04,  2.9722e-01,  3.1774e-01,  3.6227e-02,\n",
      "        -4.5781e-02, -6.7220e-01, -9.5073e-01,  2.3168e-01, -1.8188e-01,\n",
      "        -4.4898e-01,  6.1648e-01, -5.1101e-01, -2.3893e-01, -2.6587e-01,\n",
      "        -2.2783e-01, -1.2491e-02,  4.5949e-04,  1.6282e-03, -5.3697e-01,\n",
      "         3.7148e-02,  2.3255e-02, -2.7411e-01, -1.3709e-01,  5.2853e-01,\n",
      "        -4.7033e-01, -2.8181e-01,  2.9219e-02,  1.8312e-02, -2.4518e-02,\n",
      "        -5.3104e-01, -2.5274e-01, -5.4624e-01,  2.7611e-01,  2.0230e-01,\n",
      "        -1.6112e-01, -2.6318e-02,  6.9930e-02,  4.1207e-01, -1.9004e-02,\n",
      "         3.9578e-02, -1.7468e-01, -2.2365e-01,  1.3560e-01, -6.8245e-01,\n",
      "         6.2238e-01,  3.1666e-01,  4.2294e-01, -3.9493e-01,  2.3206e-01,\n",
      "        -4.4022e-02,  4.5422e-01,  1.4601e-03, -4.2992e-01,  5.7284e-01,\n",
      "        -2.2541e-02, -4.4242e-02, -6.6299e-01, -7.1049e-01, -1.6838e-01,\n",
      "        -4.7525e-01,  2.8963e-02, -9.7971e-02,  1.2029e-01, -1.8252e-01,\n",
      "        -7.0273e-01,  2.4733e-01,  1.4679e-01, -3.3418e-01, -2.3207e-02,\n",
      "         8.0534e-01,  8.2125e-01, -6.9490e-01,  2.4095e-01, -2.8442e-01,\n",
      "        -8.8784e-01, -4.8070e-01, -1.3540e-01, -5.5552e-01, -3.1151e-01,\n",
      "         1.3868e-01, -3.7624e-01, -4.2803e-01,  7.3066e-01, -2.9609e-01,\n",
      "        -6.4255e-01, -1.0851e-01, -7.0847e-01,  9.1790e-01, -4.7749e-03,\n",
      "        -1.8899e-01, -5.1284e-01, -3.0179e-02, -6.0723e-01, -5.5376e-01,\n",
      "         1.0872e+00, -3.2864e-01, -5.1895e-01, -4.0533e-01, -3.8992e-01,\n",
      "        -9.0945e-01, -5.4121e-02, -3.9828e-01, -4.9237e-01, -3.0040e-01,\n",
      "        -2.0306e-01,  9.3352e-01,  1.0400e+00,  4.5085e-01, -2.0061e-01,\n",
      "         6.3900e-01, -2.1762e-01, -6.8916e-01,  1.3119e-01, -5.4438e-01,\n",
      "        -4.6017e-01, -3.8809e-01, -4.9021e-01, -5.1690e-02, -3.3622e-02,\n",
      "        -1.5256e-01,  1.0672e-01, -3.7294e-01, -6.0778e-01, -5.9239e-01,\n",
      "        -3.3838e-01, -2.7696e-01,  2.7562e-01, -7.9161e-01, -4.1132e-01,\n",
      "         7.5320e-01, -2.2625e-01,  4.7494e-01, -1.2017e-01, -3.9686e-01,\n",
      "        -3.3877e-01, -2.5927e-01, -3.7284e-01, -1.9255e-01, -2.3438e-01,\n",
      "         3.9092e-01, -1.5930e-01, -2.4891e-01, -5.5321e-01,  9.1852e-01,\n",
      "        -6.3847e-02, -8.7602e-01, -1.2756e-01,  3.3405e-01, -1.4249e-01,\n",
      "        -2.4818e-01, -6.8456e-01, -6.9603e-01,  5.0048e-01, -1.9840e-01,\n",
      "        -1.8367e-01,  1.7114e-01,  4.1525e-02,  8.0279e-01,  5.3640e-01,\n",
      "        -7.0698e-01,  4.8994e-01,  5.3418e-01,  4.9334e-02, -1.8518e-02,\n",
      "         2.5087e-01,  1.7777e-01,  6.3259e-01,  4.7797e-01,  6.5864e-01,\n",
      "        -1.1376e-01,  5.0447e-02,  5.3073e-02,  4.9040e-01, -2.2214e-01,\n",
      "         4.0664e-01, -8.9347e-02, -4.8524e-01,  5.1639e-01, -1.4436e-01,\n",
      "         5.2397e-01,  1.1460e-01, -4.1420e-01, -3.4307e-01,  5.2815e-01,\n",
      "        -1.2766e-01,  3.9896e-02,  4.5622e-01,  1.5981e-01, -3.4112e-01,\n",
      "         4.6190e-01,  4.4852e-01,  2.3795e-02,  1.4402e-01, -3.2478e-01,\n",
      "         2.8716e-01, -5.1665e-01, -3.8279e-01, -5.4106e-01,  3.2749e-01,\n",
      "        -4.9205e-02, -2.4323e-01,  3.7841e-01, -5.4024e-02, -1.5412e-01,\n",
      "         5.0113e-01, -4.0785e-02, -2.2028e-02, -6.1386e-02, -3.1250e-01,\n",
      "         1.2265e-01,  1.2279e-01, -1.1636e-01, -6.6843e-01,  1.2983e-01,\n",
      "         3.5090e-01, -5.4123e-01,  2.1010e-01,  7.5499e-02, -2.4941e-01,\n",
      "         5.4157e-01,  3.4560e-01,  4.0658e-01, -6.3187e-01,  3.3324e-03,\n",
      "        -3.0915e-01,  1.7680e-01, -2.4794e-01,  4.4581e-02, -2.8751e-01,\n",
      "        -6.1198e-02,  4.8369e-01,  8.2405e-02, -5.3262e-01,  1.6486e-01,\n",
      "         4.7992e-01, -5.9763e-01, -1.3592e-01, -5.2884e-01, -2.2644e-01,\n",
      "         2.4885e-01,  3.2271e-01,  2.8389e-01,  5.7577e-01,  2.0099e-01,\n",
      "        -4.7723e-01, -3.8827e-01,  4.7989e-01, -2.2222e-01, -3.5789e-01,\n",
      "        -3.1538e-01, -7.6257e-03,  5.4993e-01,  3.1439e-01, -5.2481e-01,\n",
      "        -4.8050e-01,  2.3838e-01,  4.3077e-01, -1.0534e-01, -1.5628e-01,\n",
      "         4.8653e-01,  3.3297e-01,  7.1565e-01,  9.2206e-01,  7.1575e-01,\n",
      "        -7.6110e-01,  6.2438e-01,  5.5336e-01,  3.9336e-01, -1.4510e+00,\n",
      "         1.8770e+00,  1.0760e+00, -4.6316e-02, -6.3193e-01, -5.2024e-02,\n",
      "        -3.9023e-01,  1.4423e-01, -1.1208e+00, -5.9480e-01, -1.2649e+00,\n",
      "        -4.9291e-01,  6.2590e-01,  2.5391e-01,  8.3678e-01, -7.7612e-01,\n",
      "        -3.2762e-02,  8.2249e-01,  1.0040e+00,  1.8284e+00,  8.5076e-01,\n",
      "        -4.9936e-02, -1.3938e+00,  6.3167e-01,  9.0749e-03,  4.8078e-01,\n",
      "         1.6444e+00,  6.1681e-01, -9.8136e-01, -7.2676e-02,  3.4438e-01,\n",
      "         1.6998e-01,  2.2034e-01,  2.7746e-02,  2.0606e-01, -1.6887e+00,\n",
      "        -7.3376e-01,  1.9285e+00,  2.0213e+00,  9.4631e-01, -1.0906e+00,\n",
      "         1.4906e+00,  1.1975e+00, -1.2189e+00,  2.2188e-01, -9.2563e-01,\n",
      "        -4.3868e-01,  4.3390e-02, -1.5182e+00,  7.8184e-01,  4.2421e-01,\n",
      "        -1.8026e+00,  2.6242e-01, -1.5064e+00, -2.7286e-01,  5.7509e-01,\n",
      "         6.2283e-01, -1.3559e-01,  8.6225e-01, -5.3512e-01, -5.5057e-01,\n",
      "         1.4011e+00, -1.2859e+00, -1.6001e-01, -4.9871e-01,  3.8862e-01,\n",
      "        -6.2457e-01, -4.7055e-02,  7.0100e-01,  3.5888e-01, -1.4262e+00,\n",
      "         1.4369e+00,  1.0205e+00, -1.4350e+00,  2.4622e-02,  1.7029e+00,\n",
      "         7.6038e-01,  5.1966e-01, -1.2001e+00,  3.6298e-01,  7.6254e-01,\n",
      "        -1.0074e+00,  4.0510e-01, -4.8610e-01,  1.4204e+00,  7.1962e-01,\n",
      "         3.3631e-01,  7.3439e-01, -1.9508e-01,  1.7234e+00,  1.2291e+00],\n",
      "       device='cuda:0')), ('lstm.bias_hh_l0', tensor([-2.7921e-01,  2.0057e-01,  2.6921e-01, -3.6753e-01,  1.1280e-01,\n",
      "        -1.9349e-01, -5.9317e-01,  3.8460e-02, -5.6851e-02, -9.4275e-02,\n",
      "        -4.9519e-01, -9.1887e-01,  1.6874e-01, -1.5288e-02,  1.2126e-01,\n",
      "        -1.9597e-01, -1.8102e-01, -2.2734e-01,  1.5444e-01, -1.6353e-01,\n",
      "        -2.4423e-01,  1.8885e-01,  4.2250e-01, -8.5279e-01, -1.6512e-01,\n",
      "        -1.3109e-01,  3.2587e-01, -9.2585e-02, -8.3722e-01, -9.1811e-02,\n",
      "        -2.3892e-01,  5.8235e-03, -4.5689e-02,  1.8527e-02,  3.5208e-01,\n",
      "        -1.0422e+00,  4.9984e-01, -1.9443e-01,  4.5568e-01,  3.0721e-02,\n",
      "         2.4971e-01, -2.9767e-02,  1.6994e-01,  3.6746e-01,  1.3444e-01,\n",
      "         5.8190e-02, -7.2690e-01, -9.3900e-01,  1.7495e-01, -2.4082e-01,\n",
      "        -5.4309e-01,  6.2814e-01, -4.4746e-01, -3.1168e-01, -2.2475e-01,\n",
      "        -1.6671e-01, -1.8488e-02, -9.9719e-02, -1.1439e-01, -5.1252e-01,\n",
      "        -2.8952e-02, -4.3007e-02, -2.3413e-01, -5.5173e-02,  5.3844e-01,\n",
      "        -5.5696e-01, -1.6848e-01,  1.2542e-01, -4.8126e-02, -2.3542e-02,\n",
      "        -4.6444e-01, -1.8164e-01, -7.0360e-01,  3.0334e-01,  1.8496e-01,\n",
      "        -1.6214e-01, -4.3925e-02,  2.1567e-01,  4.0502e-01, -5.0441e-02,\n",
      "        -6.7597e-02, -2.3366e-01, -4.5404e-02,  1.3099e-01, -8.0892e-01,\n",
      "         4.6275e-01,  3.8459e-01,  3.1948e-01, -3.8729e-01,  1.7501e-01,\n",
      "        -1.1714e-01,  2.7899e-01,  8.7024e-02, -4.4113e-01,  6.6380e-01,\n",
      "         1.5147e-02, -7.8448e-02, -7.5541e-01, -5.4281e-01, -1.9480e-01,\n",
      "        -4.0466e-01, -1.3982e-01, -2.2251e-01,  2.5609e-01, -5.2719e-02,\n",
      "        -6.6076e-01,  1.4716e-01,  2.5103e-01, -4.8600e-01,  4.9847e-02,\n",
      "         8.6507e-01,  8.9323e-01, -7.0561e-01,  4.0324e-01, -3.3657e-01,\n",
      "        -9.7734e-01, -4.4871e-01, -1.5481e-01, -6.8387e-01, -3.5625e-01,\n",
      "         5.8132e-02, -2.6337e-01, -5.1299e-01,  6.1740e-01, -1.8370e-01,\n",
      "        -5.5677e-01, -1.3991e-01, -7.6643e-01,  1.0802e+00,  1.2532e-01,\n",
      "        -2.7143e-01, -4.7966e-01,  5.8050e-03, -5.8477e-01, -6.3815e-01,\n",
      "         1.1695e+00, -3.7460e-01, -4.6510e-01, -4.8995e-01, -3.8411e-01,\n",
      "        -8.9768e-01, -1.1946e-01, -4.1407e-01, -5.2465e-01, -2.0958e-01,\n",
      "        -1.4493e-01,  9.5994e-01,  1.1025e+00,  5.8310e-01, -3.1723e-01,\n",
      "         6.4128e-01, -8.4336e-02, -8.0368e-01,  1.3242e-01, -5.8088e-01,\n",
      "        -3.4289e-01, -2.6267e-01, -5.5144e-01, -1.0687e-01,  1.2665e-02,\n",
      "        -1.0310e-01,  1.8331e-02, -3.4660e-01, -5.9916e-01, -4.4671e-01,\n",
      "        -1.4691e-01, -2.1541e-01,  2.5019e-01, -8.3313e-01, -4.8228e-01,\n",
      "         6.9780e-01, -2.8330e-01,  5.4113e-01, -1.8961e-01, -2.9719e-01,\n",
      "        -2.9820e-01, -2.2765e-01, -4.5769e-01, -1.4123e-01, -3.3729e-01,\n",
      "         3.5013e-01, -2.2637e-02, -3.9862e-01, -5.7448e-01,  1.0232e+00,\n",
      "        -1.1489e-01, -8.5920e-01, -8.0831e-02,  4.8032e-01, -2.3003e-01,\n",
      "        -2.9087e-01, -6.0086e-01, -7.7476e-01,  5.1094e-01, -2.7960e-01,\n",
      "        -2.8109e-01,  3.0885e-01,  1.4948e-01,  8.1810e-01,  4.4332e-01,\n",
      "        -6.6210e-01,  4.2148e-01,  4.7388e-01, -1.8992e-02, -3.7129e-02,\n",
      "         1.5595e-01,  5.5757e-02,  6.5569e-01,  5.2117e-01,  5.7479e-01,\n",
      "        -4.4647e-02,  7.0954e-02, -5.5176e-02,  5.4527e-01, -3.0180e-01,\n",
      "         3.1627e-01, -1.3619e-01, -3.7794e-01,  5.9207e-01, -2.5919e-01,\n",
      "         5.3491e-01,  1.9170e-01, -4.5404e-01, -2.2414e-01,  4.3920e-01,\n",
      "        -2.3392e-01, -2.2769e-02,  3.4140e-01,  1.5009e-01, -1.9733e-01,\n",
      "         4.4553e-01,  4.3665e-01, -1.4315e-01,  1.2782e-01, -3.0268e-01,\n",
      "         1.3733e-01, -4.8522e-01, -4.0964e-01, -6.0031e-01,  5.0187e-01,\n",
      "        -1.4304e-01, -1.0464e-01,  3.3124e-01, -1.3255e-01, -8.1654e-02,\n",
      "         5.9078e-01,  3.6870e-02, -2.5591e-02,  1.3050e-01, -3.1795e-01,\n",
      "         4.4171e-02,  2.0564e-01, -2.6233e-01, -5.4712e-01,  8.3986e-02,\n",
      "         4.2305e-01, -6.1563e-01,  2.2818e-01,  6.0625e-02, -3.8638e-01,\n",
      "         6.0849e-01,  3.9726e-01,  3.5564e-01, -5.8707e-01,  6.5267e-03,\n",
      "        -3.2938e-01,  3.0129e-01, -1.1685e-01, -4.8307e-02, -4.5263e-01,\n",
      "        -1.4765e-01,  4.6579e-01,  1.1380e-01, -5.2181e-01,  1.9810e-01,\n",
      "         4.8194e-01, -4.4883e-01, -2.0244e-01, -4.8957e-01, -2.0940e-01,\n",
      "         1.4716e-01,  3.1672e-01,  2.6226e-01,  6.0258e-01,  4.5585e-02,\n",
      "        -5.5129e-01, -3.6746e-01,  4.5047e-01, -1.5364e-01, -3.1893e-01,\n",
      "        -4.1303e-01,  9.0920e-02,  5.7220e-01,  3.5193e-01, -5.6522e-01,\n",
      "        -6.4819e-01,  2.5329e-01,  2.8298e-01, -9.0777e-02, -9.8011e-02,\n",
      "         6.5434e-01,  2.8017e-01,  7.5272e-01,  1.0447e+00,  7.6368e-01,\n",
      "        -8.1855e-01,  7.5147e-01,  6.3186e-01,  2.5455e-01, -1.4343e+00,\n",
      "         1.7390e+00,  1.1309e+00, -1.3852e-01, -6.9816e-01,  4.2686e-02,\n",
      "        -3.2453e-01,  1.5475e-01, -1.1256e+00, -6.0062e-01, -1.1983e+00,\n",
      "        -4.7030e-01,  5.9785e-01,  2.6228e-01,  8.2981e-01, -8.4977e-01,\n",
      "        -5.4284e-02,  9.4668e-01,  1.1006e+00,  1.6900e+00,  8.5145e-01,\n",
      "        -2.3608e-02, -1.3032e+00,  5.7448e-01,  1.6185e-01,  3.7383e-01,\n",
      "         1.5556e+00,  6.6994e-01, -9.8046e-01, -2.9532e-02,  5.0779e-01,\n",
      "         4.0942e-02,  3.9200e-01, -1.1171e-03,  2.3789e-01, -1.6147e+00,\n",
      "        -8.5760e-01,  1.9090e+00,  1.9531e+00,  9.2732e-01, -1.1985e+00,\n",
      "         1.5454e+00,  1.2893e+00, -1.2244e+00,  1.8392e-01, -7.2949e-01,\n",
      "        -4.0251e-01,  9.0084e-02, -1.6248e+00,  9.4630e-01,  4.0738e-01,\n",
      "        -1.7169e+00,  3.4017e-01, -1.4373e+00, -2.7027e-01,  6.0230e-01,\n",
      "         7.4614e-01, -8.2408e-02,  8.3656e-01, -5.5752e-01, -6.0806e-01,\n",
      "         1.4499e+00, -1.3502e+00, -3.0308e-04, -5.2371e-01,  4.2603e-01,\n",
      "        -6.3610e-01, -1.9854e-01,  7.3643e-01,  2.5241e-01, -1.4154e+00,\n",
      "         1.4610e+00,  1.0621e+00, -1.4405e+00,  1.5236e-02,  1.6832e+00,\n",
      "         8.1000e-01,  4.5610e-01, -1.2188e+00,  3.4978e-01,  7.6174e-01,\n",
      "        -8.6978e-01,  5.1279e-01, -5.0019e-01,  1.4067e+00,  6.2276e-01,\n",
      "         3.5043e-01,  6.8103e-01, -1.7835e-01,  1.8028e+00,  1.1386e+00],\n",
      "       device='cuda:0')), ('linear.weight', tensor([[-0.0684,  0.0548,  0.3286,  ..., -0.0150, -0.0400, -0.0186],\n",
      "        [ 0.2520, -0.1610, -1.0431,  ..., -1.1770,  0.2431,  0.2948],\n",
      "        [ 0.7051,  0.4799,  0.8053,  ...,  0.0172, -0.0820, -0.1305],\n",
      "        ...,\n",
      "        [-0.2233, -0.3215, -0.6122,  ..., -1.8663,  0.4453,  0.0900],\n",
      "        [ 0.0354, -0.2038, -0.3569,  ..., -1.3754,  0.3093,  0.2015],\n",
      "        [ 0.2622, -0.2650, -0.5861,  ..., -1.4823,  0.1757,  0.2480]],\n",
      "       device='cuda:0')), ('linear.bias', tensor([ 3.7034, -4.2463,  2.6216,  ..., -1.5979, -0.6568, -0.6866],\n",
      "       device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "print(torch.load('language_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用最好的模型在valid数据上计算perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity:  215.39051555023698\n"
     ]
    }
   ],
   "source": [
    "val_loss=evaluate(best_model, val_iter)\n",
    "print('perplexity: ', np.exp(val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用最好的模型在测试数据上计算perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity:  266.89665179177774\n"
     ]
    }
   ],
   "source": [
    "test_loss=evaluate(best_model, test_iter)\n",
    "print('perplexity: ', np.exp(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用训练好的模型生成一些句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x000001FF2409FA08>>\n",
      "history of bulletin how to efficacy for their <unk> paperback inc on the planet s showed at the science of westminster acclaim one eight four five located in several one seven zero of the five zero zero zero run away from once rating at first prominence reached rank the rhineland one nine seven three besides the union s two three five th counties of sicily one nine six one in fallen during world war ii charitable pool ends with negotiated larger audiences of a peace treaty the nearby only allowed to consonants against the curtain gradually indentured papal court who\n"
     ]
    }
   ],
   "source": [
    "hidden=best_model.init_hidden(1)\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_word = torch.randint(VOCAB_SIZE, (1, 1), dtype=torch.long).to(device)\n",
    "print(input)\n",
    "words=[]\n",
    "\n",
    "for i in range(100):\n",
    "    #run forwaard pass\n",
    "    output, hidden=best_model(input_word,hidden)\n",
    "    if i==0:\n",
    "        print(output.shape)\n",
    "    #logits exp\n",
    "    word_weights=output.squeeze().exp().cpu()\n",
    "    #multinomial ssampling\n",
    "    word_idx=torch.multinomial(word_weights, 1)[0] #greddy (argmax)\n",
    "    #fill in the current predicted word to the current input\n",
    "    input_word.fill_(word_idx)\n",
    "    word=TEXT.vocab.itos[word_idx]\n",
    "    words.append(word)\n",
    "print(' '.join(words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pytorch",
   "language": "python",
   "name": "venv_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
